{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Sentiment Analysis\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "__BACKGROUND__\n",
    "\n",
    "A large multinational corporation is seeking to automatically identify the sentiment that their customer base talks about on social media. They would like to expand this capability into multiple languages. Many 3rd party tools exist for sentiment analysis, however, they need help with under-resourced languages.\n",
    "\n",
    "__GOAL__\n",
    "\n",
    "Train a sentiment classifier (Positive, Negative, Neutral) on a corpus of the provided documents. Your goal is to maximize accuracy. There is special interest in being able to accurately detect negative sentiment. The training data includes documents from a wide variety of sources, not merely social media, and some of it may be inconsistently labeled. Please describe the business outcomes in your work sample including how data limitations impact your results and how these limitations could be addressed in a larger project.\n",
    "\n",
    "__DATA__ \n",
    "\n",
    "Link to data: http://archive.ics.uci.edu/ml/datasets/Roman+Urdu+Data+Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Recommendations \n",
    "\n",
    "## Model Description\n",
    "\n",
    "We have developed a model that was able to achieve an overall accuracy of 63% accuracy, with a recall of 50% for negative sentiments. Per the problem statement, we optimized for overall accuracy. However, the model can be adjusted to increase sensitivity to \"Negative\" statements, albeit at the expense of overall accuracy. For example, if we would like to increase our negative sentiment recall to 67%, we decrease overall accuracy to 58%. "
   ]
  },
  {
   "attachments": {
    "tradeoff.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tradeoff.jpeg](attachment:tradeoff.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is based on an LSTM architecture, a type of neural network that performs well for data for which order is important, such as the order of words in text. We first explored a simpler \"bag of words\" approach that aims to understand the sentiment of each word and then considers the sentiment of the text to be the average sentiment of all its words. This approach performed very poorly, which led us to the more sophisticated LSTM model. \n",
    "\n",
    "An overview of our model development is below with all details in the appendix.\n",
    "\n",
    "While the LSTM model performed markedly better than the \"bag of words\" model, its performance is still unsatisfactory. We have compile a list of several recommendations for improving this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology Overview\n",
    "\n",
    "In _Apppendix: Methodology_ we provide details - including all code - for the process by which we created the sentiment model. \n",
    "\n",
    "Here we provide a brief description of the process.\n",
    "\n",
    "1. __Data Preparation:__ We loaded data into a DataFrame, corrected obvious data errors, and explored various properties of the data features. Of note, the text includes not just Romanized Urdu but also English, emoji's, empty text fields, and non-standard usage of punctuation and capitalization. \n",
    "\n",
    "2. __Text Parsing:__ With no prior knowledge of Urdu, we used default parsing techniques. We moved all text to lower case, broke sentences at end of word punctuation, and broke words at whitespace. We then removed remaining punctuation. Thus sentences consisted of lists of lower cased, punctuationless words and documents consisted of lists of sentences. \n",
    "\n",
    "3. __Word Embedding:__ We will be exploring embedding based solutions to the sentiment analysis problem so we build a word embedding using the word2vec algorithm. This is a standard option with a fast implementation.\n",
    "\n",
    "4. __Model Exploration and Selection:__ We explore two model options: a \"bag of words\" based model and an LSTM based model. The \"bag of words\" approach learns sentiment based on the average of the embeddings of the words in the document. The LSTM based approach learns to sentiment by reading the sequence of words in sentence. The primary advantage of the \"bag of words\" approach is that it requires little data to learn. The LSTM model can achieve much better performance on sufficiently sized data. In our case, the LSTM outperformed the \"bag of words\" approach \n",
    "across a variety of parameters on the test data. \n",
    "\n",
    "5. __Model Tuning:__ We tuned the \"LSTM\" model against a variety of parameters. \n",
    "\n",
    "6. __Model Evaluation:__ We evaluate accuracy and \"Negative\" recall at a variety of classification thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Model Improvement\n",
    "\n",
    "In the course of constructing a sentiment classifier, we developed several recommendations that could improve the classifier.\n",
    "\n",
    "- __Identify social media vs other__ Different media have different language usage. By developing a model based on non social media content we are essentially performing transfer learning (applying training from one data distribution to another). While transfer learning can be useful to augment a small data set, it can introduce bias into models.\n",
    "\n",
    "- __Include time stamps__ Similarly, language usage shifts over time. By including a time component, we can identify model drift. \n",
    "\n",
    "- __Basic Urdu knowledge__ While much of language structure can be learned using a deep learning approach, we are helped by basic knowledge of language structure. Common text processing techniques for English such as stemming and parsing words by whitespace are based on a priori rules. In our work we used a minimal set of \"reasonable\" parsing rules, but greater confidence in these rules would be helpful. \n",
    "\n",
    "- __Get access to Urdu Embedding__ At the base of our model development was the creation of a word embedding. Access to a high quality embedding trained on a much larger Roman Urdu data set could be extremely useful.\n",
    "\n",
    "- __Language classifier__ Some of the texts were exclusively in English. If we had a high quality English language classifier, we could apply English sentiment classifiers to those texts and remove them from our training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/cfrailey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random as random\n",
    "random.seed(5) \n",
    "import os\n",
    "import keras as k\n",
    "import csv\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "np.random.seed(34)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import itertools\n",
    "from gensim.test.utils import get_tmpfile, datapath\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Inspection\n",
    "\n",
    "We first explore the data. We have been warned that the training data \"includes documents from a wide variety of sources, not merely social media, and some of it may be inconsistently labeled\" and we want to understand the implications of these challenges. \n",
    "\n",
    "We inspect data shape, sample the data, and look at basic column statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load data as DataFrame using default encoding\n",
    "data = pd.read_csv('RomanUrduDataSet.csv',sep=',',names=['text','sentiment'],index_col=False, \n",
    "                  quotechar='\"' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20229, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18456</th>\n",
       "      <td>ALLAH THAALLAH HAM SAB P RAHEM FARMAY AMEEN SU...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>Iss News ka Source kia hay.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Jizroor ..... Mei apne aunty  ka dilse Shukriy...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>hahahhaah</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6358</th>\n",
       "      <td>Pir Pagara ne Muslim University Aligarh ke il...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>Edhi Foundation Ambulance Service app kay beta...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>Marka e Kargill ke dosre Nishan e Haider, Hava...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15950</th>\n",
       "      <td>Plz mery liye b dua kariyga Mera Allah mjhsy R...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>Plz dow kar da mary or mary ghar waloo ka lia</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>to gussy main kapre e mujhe de ti thi k ja ab ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "18456  ALLAH THAALLAH HAM SAB P RAHEM FARMAY AMEEN SU...  Positive\n",
       "7864                         Iss News ka Source kia hay.   Neutral\n",
       "274    Jizroor ..... Mei apne aunty  ka dilse Shukriy...  Positive\n",
       "5037                                           hahahhaah   Neutral\n",
       "6358    Pir Pagara ne Muslim University Aligarh ke il...   Neutral\n",
       "1185   Edhi Foundation Ambulance Service app kay beta...  Positive\n",
       "5932   Marka e Kargill ke dosre Nishan e Haider, Hava...   Neutral\n",
       "15950  Plz mery liye b dua kariyga Mera Allah mjhsy R...  Positive\n",
       "16051      Plz dow kar da mary or mary ghar waloo ka lia  Positive\n",
       "10341  to gussy main kapre e mujhe de ti thi k ja ab ...  Negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20228</td>\n",
       "      <td>20229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19664</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Good</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>23</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text sentiment\n",
       "count   20228     20229\n",
       "unique  19664         4\n",
       "top      Good   Neutral\n",
       "freq       23      8929"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         1\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'null' text with ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     8929\n",
       "Positive    6013\n",
       "Negative    5286\n",
       "Neative        1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are fairly balanced. \n",
    "However we need to correct mislabeled 'Neative' to 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['sentiment'] == 'Neative', 'sentiment'] = 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16177</th>\n",
       "      <td>Allah apko jazaye khair dy hazrat sb</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11150</th>\n",
       "      <td>Baldia Muwach Goth Me Chhurio K War Se Shabana...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717</th>\n",
       "      <td>good man</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>abhi jab main Russia mein hone wale World Univ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Allah ka shuakr hy ye tw hum per ðŸ˜‚</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14489</th>\n",
       "      <td>Kya ye me b kr lu</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12053</th>\n",
       "      <td>Insaf ki farhami ki sorat e haal ye hai ke ko...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2781</th>\n",
       "      <td>1963 mein unhon ne apney adbi resalaï¿½EEEâ‚¬ï¿½EEEF...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14965</th>\n",
       "      <td>bichary ko 2 chaperein paren</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13104</th>\n",
       "      <td>tv drammas mein itna khuchh bakwas ho raha hai...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>Usi dauran inho ne University of Southern Cali...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16391</th>\n",
       "      <td>2 rakat hajat aur 2 rakat touba k total 4 raha...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>thek kaha</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10085</th>\n",
       "      <td>&lt;3 CH ADNAN ANSAR GUJJAR &lt;3</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>Main janti hon ke mere munfarid andaaz hosting...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>Wo 1930 mein riha hue to watan ki azaadi ke l...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13895</th>\n",
       "      <td>So b jaya kr</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15455</th>\n",
       "      <td>Husband k liya khuch batay qaboo karny k liya</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13860</th>\n",
       "      <td>Kon sa scene</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8962</th>\n",
       "      <td>Okey brov</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15445</th>\n",
       "      <td>Drama to ajeeb he kedia .... Startng main acha...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19419</th>\n",
       "      <td>Abhay ditto nai. YE WOHI HAI</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>1950 mein aap ko Aqwam-e-Mutehda mein Pakistan...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>1992 se 1993 ke doran Salman ki 6 filmein naka...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11593</th>\n",
       "      <td>Rahai mili to chand mah bad 2 mah key leyae Ka...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8455</th>\n",
       "      <td>Hahah ok g</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15769</th>\n",
       "      <td>Dua farma dain meri sehat k luay aur meray baa...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17264</th>\n",
       "      <td>Lant es pr r es ke nasl pr</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9352</th>\n",
       "      <td>Chaye hr session ma last number pr aye phir b ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>Pakistan Ka SriLanka K Khilaaf Toss Jeet Kar B...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7306</th>\n",
       "      <td>Jin mein science fiction  adventure  jangi  sa...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7747</th>\n",
       "      <td>KPK clerical all post upgradation May 2014</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>Pakistani cricket team nay 1992 mein Imran Kha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>please bataao</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td>Issi saal mahkama excise  (excise department) ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Bilkul sahi kaha</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>Nahi ye galat he ..Allah humen 70 maa se b ziy...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13571</th>\n",
       "      <td>Kysy bnay ya tyl</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9840</th>\n",
       "      <td>Shikra</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>Bohat achi ghari hai. Iski tou price bhee boha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>MQM K Rukn e Assambly Landhi Mn Awami Ahtijaj ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>3 February 1997 kay intekhabat (election) mein...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>HAHA</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17841</th>\n",
       "      <td>Tm chahte ho k mje crush k n se nafrat ho jaye</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17603</th>\n",
       "      <td>Kal q nahi hohi</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12412</th>\n",
       "      <td>Bil aakhir 24 July 1975 ko yeh jorhi toot gai</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14787</th>\n",
       "      <td>Hyy khuda bss y Kinza manhoos khee dfaa ho jae...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>Iss surat hal mein peopleâ€™s party nay maidan ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13389</th>\n",
       "      <td>Pounds ka Same Product local market mien R.s 2...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>Meray Baad Rabta Committee Party Sambhalay</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>Acha jee ðŸ˜‚</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8274</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>BuahhhhahhahahhahahahahEk ye line maze ki heOr...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>Dono actresses ka kehna hai ke inhe offers to ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>ma a k aunty ko thanks kahon giðŸ˜œ</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10372</th>\n",
       "      <td>Tuu bhii khaaabeeess hai ðŸ”¥</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>In ki piadaish ke aik baras bad hi un ke walde...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>Flavour sent nahi kiye jhot bolny lagae hain y...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "16177               Allah apko jazaye khair dy hazrat sb   Neutral\n",
       "11150  Baldia Muwach Goth Me Chhurio K War Se Shabana...  Negative\n",
       "3717                                            good man  Positive\n",
       "2689   abhi jab main Russia mein hone wale World Univ...  Positive\n",
       "235                   Allah ka shuakr hy ye tw hum per ðŸ˜‚  Positive\n",
       "14489                                  Kya ye me b kr lu   Neutral\n",
       "12053   Insaf ki farhami ki sorat e haal ye hai ke ko...  Negative\n",
       "2781   1963 mein unhon ne apney adbi resalaï¿½EEEâ‚¬ï¿½EEEF...  Positive\n",
       "14965                       bichary ko 2 chaperein paren  Negative\n",
       "8208                                                 ...   Neutral\n",
       "13104  tv drammas mein itna khuchh bakwas ho raha hai...  Negative\n",
       "2431   Usi dauran inho ne University of Southern Cali...  Positive\n",
       "16391  2 rakat hajat aur 2 rakat touba k total 4 raha...   Neutral\n",
       "9709                                           thek kaha   Neutral\n",
       "10085                        <3 CH ADNAN ANSAR GUJJAR <3   Neutral\n",
       "12326  Main janti hon ke mere munfarid andaaz hosting...  Negative\n",
       "1801    Wo 1930 mein riha hue to watan ki azaadi ke l...  Positive\n",
       "13895                                      So b jaya kr   Negative\n",
       "15455      Husband k liya khuch batay qaboo karny k liya  Negative\n",
       "13860                                       Kon sa scene   Neutral\n",
       "8962                                           Okey brov   Neutral\n",
       "15445  Drama to ajeeb he kedia .... Startng main acha...  Negative\n",
       "19419                      Abhay ditto nai. YE WOHI HAI    Neutral\n",
       "2955   1950 mein aap ko Aqwam-e-Mutehda mein Pakistan...  Positive\n",
       "7149   1992 se 1993 ke doran Salman ki 6 filmein naka...   Neutral\n",
       "11593  Rahai mili to chand mah bad 2 mah key leyae Ka...  Negative\n",
       "8455                                          Hahah ok g   Neutral\n",
       "15769  Dua farma dain meri sehat k luay aur meray baa...  Positive\n",
       "17264                         Lant es pr r es ke nasl pr  Negative\n",
       "9352   Chaye hr session ma last number pr aye phir b ...   Neutral\n",
       "5361   Pakistan Ka SriLanka K Khilaaf Toss Jeet Kar B...   Neutral\n",
       "7306   Jin mein science fiction  adventure  jangi  sa...   Neutral\n",
       "7747          KPK clerical all post upgradation May 2014   Neutral\n",
       "1867   Pakistani cricket team nay 1992 mein Imran Kha...  Positive\n",
       "5220                                       please bataao   Neutral\n",
       "7494   Issi saal mahkama excise  (excise department) ...   Neutral\n",
       "429                                     Bilkul sahi kaha  Positive\n",
       "3651   Nahi ye galat he ..Allah humen 70 maa se b ziy...  Positive\n",
       "13571                                   Kysy bnay ya tyl   Neutral\n",
       "9840                                              Shikra   Neutral\n",
       "1603   Bohat achi ghari hai. Iski tou price bhee boha...  Positive\n",
       "11522  MQM K Rukn e Assambly Landhi Mn Awami Ahtijaj ...  Negative\n",
       "6298   3 February 1997 kay intekhabat (election) mein...   Neutral\n",
       "8057                                                       Neutral\n",
       "4965                                                HAHA   Neutral\n",
       "17841     Tm chahte ho k mje crush k n se nafrat ho jaye  Negative\n",
       "17603                                    Kal q nahi hohi   Neutral\n",
       "12412      Bil aakhir 24 July 1975 ko yeh jorhi toot gai  Negative\n",
       "14787  Hyy khuda bss y Kinza manhoos khee dfaa ho jae...  Negative\n",
       "11913   Iss surat hal mein peopleâ€™s party nay maidan ...  Negative\n",
       "13389  Pounds ka Same Product local market mien R.s 2...  Negative\n",
       "5376          Meray Baad Rabta Committee Party Sambhalay   Neutral\n",
       "4783                                          Acha jee ðŸ˜‚   Neutral\n",
       "8274                                                       Neutral\n",
       "8409   BuahhhhahhahahhahahahahEk ye line maze ki heOr...   Neutral\n",
       "6858   Dono actresses ka kehna hai ke inhe offers to ...   Neutral\n",
       "4703                    ma a k aunty ko thanks kahon giðŸ˜œ   Neutral\n",
       "10372                         Tuu bhii khaaabeeess hai ðŸ”¥  Negative\n",
       "12452  In ki piadaish ke aik baras bad hi un ke walde...  Negative\n",
       "11847  Flavour sent nahi kiye jhot bolny lagae hain y...  Negative"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data a bit more\n",
    "data.sample(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a spotcheck for consistency, look for sentiment != 'Positive' and text == 'Good'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((data['text'] == 'Good') & (data['sentiment'] != 'Positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLength'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20229 entries, 0 to 20228\n",
      "Data columns (total 3 columns):\n",
      "text          20229 non-null object\n",
      "sentiment     20229 non-null object\n",
      "textLength    20229 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 474.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will look at the text distribution to see if there are clear clusters that might indicate content from social media or not. If so we can evaluate our models against these different clusters. Or we could inspect ourliers and consider how to treat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1e5549efd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot(data['textLength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1e5498f7b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data['textLength'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems obvious here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Parsing\n",
    "\n",
    "This is an area where lack of knowledge of the language can be quite a hinderance. We use what we consider a conservative approach to text parsing. \n",
    "\n",
    "We lower case, tokenize sentences using defaults, tokenize words using defaults, then strip whitespace and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UrduParsing(text):\n",
    "    # create list of sentences in text\n",
    "    sentences = nltk.sent_tokenize(text.lower())\n",
    "    # create list of lists of words in sentences and remove whitespace and punctuation\n",
    "    wordsInSentences = [nltk.word_tokenize(s) for s in sentences]\n",
    "    \n",
    "    # loop through words to remove whitespace and punctuation and remove empty words\n",
    "    cleanWordsInSentences = []\n",
    "    table = str.maketrans({key: None for key in string.punctuation+string.whitespace})\n",
    "    for s in wordsInSentences:\n",
    "        cleanSentence = []\n",
    "        for w in s:\n",
    "            cleanWord = w.translate(table)\n",
    "            if len(cleanWord) > 0:\n",
    "                cleanSentence.append(cleanWord)\n",
    "        if len(cleanSentence) > 0:\n",
    "            cleanWordsInSentences.append(cleanSentence)\n",
    "    \n",
    "    return cleanWordsInSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['parsedText'] = data.apply(lambda x: UrduParsing(x['text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>textLength</th>\n",
       "      <th>parsedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15906</th>\n",
       "      <td>yahan par personal batain na karain</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>35</td>\n",
       "      <td>[[yahan, par, personal, batain, na, karain]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6597</th>\n",
       "      <td>Musharraf Rebuff jis mein Musharraf aamriyat k...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>236</td>\n",
       "      <td>[[musharraf, rebuff, jis, mein, musharraf, aam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14566</th>\n",
       "      <td>Ena sona kiun rab ne bnaya</td>\n",
       "      <td>Positive</td>\n",
       "      <td>27</td>\n",
       "      <td>[[ena, sona, kiun, rab, ne, bnaya]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15223</th>\n",
       "      <td>Terian tak ky adavaan my Mureed ho gia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>38</td>\n",
       "      <td>[[terian, tak, ky, adavaan, my, mureed, ho, gia]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>dua Karo :p</td>\n",
       "      <td>Positive</td>\n",
       "      <td>11</td>\n",
       "      <td>[[dua, karo, p]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15002</th>\n",
       "      <td>Imran Khan ki b najaiz olad hai AP bhol rahy h...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>55</td>\n",
       "      <td>[[imran, khan, ki, b, najaiz, olad, hai, ap, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14064</th>\n",
       "      <td>bhala ho punjab walo ka</td>\n",
       "      <td>Positive</td>\n",
       "      <td>23</td>\n",
       "      <td>[[bhala, ho, punjab, walo, ka]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13690</th>\n",
       "      <td>Hota hai .. chalta haii .. facebook hai ..</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>43</td>\n",
       "      <td>[[hota, hai, chalta, haii, facebook, hai]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>Amna Abbasi ðŸ˜ƒðŸ˜ƒðŸ˜ƒðŸ˜ƒ</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>16</td>\n",
       "      <td>[[amna, abbasi, ðŸ˜ƒðŸ˜ƒðŸ˜ƒðŸ˜ƒ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18643</th>\n",
       "      <td>Aam Entehabatma any chahiye</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>27</td>\n",
       "      <td>[[aam, entehabatma, any, chahiye]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>Saen reaction dy raha hlky halky</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>32</td>\n",
       "      <td>[[saen, reaction, dy, raha, hlky, halky]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14011</th>\n",
       "      <td>Beta meray perhay ------ lagtay hayn</td>\n",
       "      <td>Negative</td>\n",
       "      <td>36</td>\n",
       "      <td>[[beta, meray, perhay, lagtay, hayn]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17447</th>\n",
       "      <td>Swaiay tayyab urdgan k tmam muslim leader .......</td>\n",
       "      <td>Negative</td>\n",
       "      <td>57</td>\n",
       "      <td>[[swaiay, tayyab, urdgan, k, tmam, muslim, lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8137</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8992</th>\n",
       "      <td>lol</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>[[lol]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13254</th>\n",
       "      <td>Bhot acha hy</td>\n",
       "      <td>Positive</td>\n",
       "      <td>12</td>\n",
       "      <td>[[bhot, acha, hy]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10711</th>\n",
       "      <td>Saray shows band karo\\n\\nKanjar khana khol k r...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>56</td>\n",
       "      <td>[[saray, shows, band, karo, kanjar, khana, kho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>Koi 3 4 saal bad main milwaon gi tmhn aisy mar...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>52</td>\n",
       "      <td>[[koi, 3, 4, saal, bad, main, milwaon, gi, tmh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14713</th>\n",
       "      <td>acha Same hero aur Heroine hai New kese</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>39</td>\n",
       "      <td>[[acha, same, hero, aur, heroine, hai, new, ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15448</th>\n",
       "      <td>raju ko chahye hai k hina ko factory k toilet ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>90</td>\n",
       "      <td>[[raju, ko, chahye, hai, k, hina, ko, factory,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "15906                yahan par personal batain na karain   Neutral   \n",
       "6597   Musharraf Rebuff jis mein Musharraf aamriyat k...   Neutral   \n",
       "14566                        Ena sona kiun rab ne bnaya   Positive   \n",
       "15223             Terian tak ky adavaan my Mureed ho gia  Positive   \n",
       "2102                                         dua Karo :p  Positive   \n",
       "15002  Imran Khan ki b najaiz olad hai AP bhol rahy h...  Negative   \n",
       "14064                            bhala ho punjab walo ka  Positive   \n",
       "13690        Hota hai .. chalta haii .. facebook hai ..    Neutral   \n",
       "4851                                    Amna Abbasi ðŸ˜ƒðŸ˜ƒðŸ˜ƒðŸ˜ƒ   Neutral   \n",
       "18643                        Aam Entehabatma any chahiye   Neutral   \n",
       "3965                    Saen reaction dy raha hlky halky   Neutral   \n",
       "14011               Beta meray perhay ------ lagtay hayn  Negative   \n",
       "17447  Swaiay tayyab urdgan k tmam muslim leader .......  Negative   \n",
       "8137                                                       Neutral   \n",
       "8992                                                 lol   Neutral   \n",
       "13254                                       Bhot acha hy  Positive   \n",
       "10711  Saray shows band karo\\n\\nKanjar khana khol k r...  Negative   \n",
       "3859   Koi 3 4 saal bad main milwaon gi tmhn aisy mar...   Neutral   \n",
       "14713            acha Same hero aur Heroine hai New kese   Neutral   \n",
       "15448  raju ko chahye hai k hina ko factory k toilet ...  Negative   \n",
       "\n",
       "       textLength                                         parsedText  \n",
       "15906          35       [[yahan, par, personal, batain, na, karain]]  \n",
       "6597          236  [[musharraf, rebuff, jis, mein, musharraf, aam...  \n",
       "14566          27                [[ena, sona, kiun, rab, ne, bnaya]]  \n",
       "15223          38  [[terian, tak, ky, adavaan, my, mureed, ho, gia]]  \n",
       "2102           11                                   [[dua, karo, p]]  \n",
       "15002          55  [[imran, khan, ki, b, najaiz, olad, hai, ap, b...  \n",
       "14064          23                    [[bhala, ho, punjab, walo, ka]]  \n",
       "13690          43         [[hota, hai, chalta, haii, facebook, hai]]  \n",
       "4851           16                             [[amna, abbasi, ðŸ˜ƒðŸ˜ƒðŸ˜ƒðŸ˜ƒ]]  \n",
       "18643          27                 [[aam, entehabatma, any, chahiye]]  \n",
       "3965           32          [[saen, reaction, dy, raha, hlky, halky]]  \n",
       "14011          36              [[beta, meray, perhay, lagtay, hayn]]  \n",
       "17447          57  [[swaiay, tayyab, urdgan, k, tmam, muslim, lea...  \n",
       "8137           11                                                 []  \n",
       "8992            3                                            [[lol]]  \n",
       "13254          12                                 [[bhot, acha, hy]]  \n",
       "10711          56  [[saray, shows, band, karo, kanjar, khana, kho...  \n",
       "3859           52  [[koi, 3, 4, saal, bad, main, milwaon, gi, tmh...  \n",
       "14713          39  [[acha, same, hero, aur, heroine, hai, new, ke...  \n",
       "15448          90  [[raju, ko, chahye, hai, k, hina, ko, factory,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look for the distribution of word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tally = {}\n",
    "for l in data['parsedText']:\n",
    "    for s in l:\n",
    "        for w in s:\n",
    "            if (w in tally.keys()):\n",
    "                tally[w] += 1\n",
    "            else:\n",
    "                tally[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1e548e1320>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(list(tally.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 33479 distince words\n",
      "19460 of those words only appear once\n",
      "26139 of those words only appear less than 4 times\n"
     ]
    }
   ],
   "source": [
    "parsedWordFrequency = pd.DataFrame(list(tally.items()), columns=('word','frequency'))\n",
    "parsedWordFrequency = parsedWordFrequency.sort_values(by = 'frequency', ascending = False)\n",
    "print('there are ' + str(parsedWordFrequency.shape[0]) + ' distince words')\n",
    "print(str(sum(parsedWordFrequency['frequency'] == 1)) + ' of those words only appear once')\n",
    "print(str(sum(parsedWordFrequency['frequency'] < 4)) + ' of those words only appear less than 4 times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "58% of words only appear once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ki</td>\n",
       "      <td>5759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>ke</td>\n",
       "      <td>5357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>mein</td>\n",
       "      <td>4367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  frequency\n",
       "7      ki       5759\n",
       "314    ke       5357\n",
       "305  mein       4367"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedWordFrequency.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pT_train, pT_test, s_train, s_test = train_test_split(data['parsedText'], data['sentiment'], \n",
    "                                                    test_size=.1, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding via Word2Vec\n",
    "\n",
    "A word embedding is a function by which words are represented as vectors in a space. The function tries to assign words to vectors in such a way that _similar_ words are sent to _close_ vectors and _dissimilar_ words are sent to _distant_ vectors. Different word embedding algorithms have different means of determining _similarity_/_dissimilarity_ and _close_/_distant_ but we chose the Word2Vec algorithm which is very popular and has a fast implementation. We will test our models' performances against different parameters for the embedding. \n",
    "\n",
    "As our corpus is quite small and we have no third party pretrained embeddings on which to evaluate our own, this could be a fruitful area for further development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Employ Word2Vec in gensim__\n",
    "\n",
    "We set _min count_ (minimum number of word occurences) of 2. This is very conservative and only removes words for which we have strongest confidence would only add noise (as we wouldn't be able to learn confident vectors for one occurence).\n",
    "\n",
    "We scan through setting _size_ (dimension of embedding space) at 20, 50, and 100.\n",
    "\n",
    "We scan through _window_ (words on either side to be considered for content-target pairs) at 2, 5, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of parsed sentences to pass to Word2Vec\n",
    "words_in_sentences = []\n",
    "for l in data['parsedText']:\n",
    "    words_in_sentences += l\n",
    "\n",
    "# Create Embeddings for sizes of 10, 50, 100 and windows of 2,5,7\n",
    "sizes = [10,50,100]\n",
    "windows = [2,5,7]\n",
    "\n",
    "word2vec = {(s,w): Word2Vec(words_in_sentences, min_count = 2, size = s, seed = 28, window = w) \n",
    "            for (s,w) in itertools.product(sizes,windows)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ameen', 0.9956144094467163),\n",
       " ('pak', 0.9951152801513672),\n",
       " ('sum', 0.9945386648178101),\n",
       " ('rakhy', 0.993998646736145),\n",
       " ('farma', 0.9939974546432495),\n",
       " ('masha', 0.9934855699539185),\n",
       " ('subhan', 0.9934506416320801),\n",
       " ('hidayat', 0.9933172464370728),\n",
       " ('salamat', 0.993302583694458),\n",
       " ('lambi', 0.993212103843689)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec[(50,7)].wv.most_similar('allah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('very', 0.9983828067779541),\n",
       " ('a', 0.9980259537696838),\n",
       " ('nice', 0.9977725744247437),\n",
       " ('with', 0.9977163076400757),\n",
       " ('it', 0.9976974129676819),\n",
       " ('i', 0.9976713061332703),\n",
       " ('so', 0.9976514577865601),\n",
       " ('u', 0.9976493716239929),\n",
       " ('and', 0.99761962890625),\n",
       " ('have', 0.9976001977920532)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec[(100,2)].wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Bag of Words Approach\n",
    "\n",
    "In this approach, each document is assigned to the center of the embeddings of the words in the document. We then train a linear function with a softmax layer to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerOfWords(words_in_sentences, vocab, w2v):\n",
    "    # Input:\n",
    "    # words_in_sentences - list of list of words representing a sequence of sentences\n",
    "    # vocab - words for which we have an embedding\n",
    "    # w2v - Word2Vec object\n",
    "    # Returns:\n",
    "    # Center of mass of words in sentences, normalized to have L2 norm of 1 if non-zero\n",
    "    \n",
    "    embed_dim = w2v[random.sample(vocab,1)].shape[1]\n",
    "    \n",
    "    center = np.zeros(embed_dim)\n",
    "    \n",
    "    for s in words_in_sentences:\n",
    "        for w in s:\n",
    "            if w in vocab:\n",
    "                center += w2v[w]\n",
    "    \n",
    "    if np.count_nonzero(center) == 0:\n",
    "        return center\n",
    "    \n",
    "    else:\n",
    "        return center/np.sqrt(np.dot(center,center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Input:\n",
    "    # x - (m,2) np.array\n",
    "    # Returns:\n",
    "    # (m,1) np.array with softmax of x over axis=1\n",
    "    \n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorify(sentiment):\n",
    "    # Input:\n",
    "    # sentiment - string\n",
    "    # Returns:\n",
    "    # One hot encoding of sentiment\n",
    "    \n",
    "    if sentiment == 'Neutral':\n",
    "        return [0,1,0]\n",
    "    elif sentiment == 'Positive':\n",
    "        return [0,0,1]\n",
    "    elif sentiment == 'Negative':\n",
    "        return [1,0,0]\n",
    "    else:\n",
    "        raise('Inappropriate Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(x):\n",
    "    # Input:\n",
    "    # x - (m,) pd.Series of sentiments\n",
    "    # Returns:\n",
    "    # (m,3) np.array of one hot encodings of sentiments\n",
    "    \n",
    "    return np.array([categorify(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropNaiveModel(X,W,b):\n",
    "    # Input:\n",
    "    # X - (m, dimEmbedding) np.array of the center of mass of the observations\n",
    "    # W,b - (dimEmbedding, 3) (1,3) np.arrays for linear transformation\n",
    "    # Returns:\n",
    "    # the (m,3) np.array resulting from the linear transformation followed by softmax\n",
    "    \n",
    "    Z = np.atleast_2d(np.dot(X,W) + b)\n",
    "    A = np.exp(Z)/np.atleast_2d(np.sum(np.exp(Z), axis = 1)).T\n",
    "    return(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNaiveModel(X,W,b,weights):\n",
    "    # Input:\n",
    "    # X - (m, dimEmbedding) np.array of the center of mass of the observations\n",
    "    # W,b - (dimEmbedding, 3) (1,3) np.arrays for linear transformation\n",
    "    # weights - (1,3) np.array to of weights to classifiers used to change classifier threshold\n",
    "    # Returns:\n",
    "    # \n",
    "    \n",
    "    A = forwardPropNaiveModel(X,W,b)*weights\n",
    "    pred = np.argmax(A,axis=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepXY(texts, sentiments, wv):\n",
    "    # Input:\n",
    "    # texts - (m,) pd.Series of texts to be converted to their center of mass\n",
    "    # sentiments - (m,) pd.Series of sentiments to be converted to one hot encodings\n",
    "    # wv - Word2Vec object\n",
    "    # Returns:\n",
    "    # X - (m, dimEmbedding) center of mass of texts\n",
    "    # Y - (m, 3) one hot encoded sentiments\n",
    "    \n",
    "    X = np.array([centerOfWords(texts.iloc[i], set(wv.vocab.keys()), wv) for i in range(texts.shape[0])])\n",
    "    Y = np.array([categorify(sentiments.iloc[i]) for i in range(sentiments.shape[0])])\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateNaiveModel(texts, sentiments, wv, W, b, weights = np.reshape(np.array([1,1,1]),(1,3))):\n",
    "    # Input:\n",
    "    # texts - (m,) pd.Series of texts\n",
    "    # sentiments - (m,) pd.Series of sentiments\n",
    "    # W,b - (dimEmbedding, 3) (1,3) np.arrays for linear transformation\n",
    "    # weights - (1,3) np.array to of weights to classifiers used to change classifier threshold\n",
    "    # Returns:\n",
    "    # confMat - (3,3) confusion matrix\n",
    "    # Ylabels - (m,) actual sentiment [0-2] for\n",
    "    # Yhat - (m,) predicted sentiment [0-2]\n",
    "    \n",
    "    X,Y = prepXY(texts, sentiments, wv)\n",
    "    Ylabels = np.argmax(Y,axis = 1)\n",
    "    Yhat = predictNaiveModel(X,W,b,weights)\n",
    "    confMat = LazyConfMat(Ylabels,Yhat)\n",
    "    return confMat,Ylabels,Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWordsmodel(X, Y, learning_rate, num_iterations):\n",
    "    # Input:\n",
    "    # X -- numpy array of sentences as strings, of shape (m, 1)\n",
    "    # Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    # word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    # learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    # num_iterations -- number of iterations   \n",
    "    # Returns:\n",
    "    # pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    # W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    # b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \n",
    "    np.random.seed(23)\n",
    "\n",
    "    m = Y.shape[0]\n",
    "    n_y = Y.shape[1]\n",
    "    n_h = X.shape[1]\n",
    "    \n",
    "    W = np.random.randn(n_h, n_y) / np.sqrt(n_h)\n",
    "    b = np.zeros((1,n_y))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        tot_cost = 0\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = np.dot(X[i,:],W) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -1*np.sum(Y[i,:]*np.log(a))\n",
    "            \n",
    "            tot_cost+= cost\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y[i,:]\n",
    "            dW = np.dot(X[i,:].reshape(n_h,1),dz.reshape(1,n_y))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(tot_cost))\n",
    "        \n",
    "    print(\"Epoch: \" + str(t) + \" --- cost = \" + str(tot_cost))\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will scan through a few learning rates\n",
    "learningRates = [.1,.01,.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 19660.817935247167\n",
      "Epoch: 9 --- cost = 19420.520848972006\n",
      "Epoch: 0 --- cost = 19466.832241487395\n",
      "Epoch: 9 --- cost = 19006.00269863968\n",
      "Epoch: 0 --- cost = 19669.893203813186\n",
      "Epoch: 9 --- cost = 19254.114209538635\n",
      "Epoch: 0 --- cost = 19628.66266522364\n",
      "Epoch: 9 --- cost = 19309.289398115445\n",
      "Epoch: 0 --- cost = 19471.331252783533\n",
      "Epoch: 9 --- cost = 18954.579366210008\n",
      "Epoch: 0 --- cost = 19695.30372040534\n",
      "Epoch: 9 --- cost = 19244.25308158631\n",
      "Epoch: 0 --- cost = 19574.14547523097\n",
      "Epoch: 9 --- cost = 19205.746989203704\n",
      "Epoch: 0 --- cost = 19414.11229642964\n",
      "Epoch: 9 --- cost = 18885.618935650207\n",
      "Epoch: 0 --- cost = 19684.167206446797\n",
      "Epoch: 9 --- cost = 19186.530191894864\n",
      "Epoch: 0 --- cost = 19737.24023569078\n",
      "Epoch: 9 --- cost = 19411.033757292014\n",
      "Epoch: 0 --- cost = 19468.522922604334\n",
      "Epoch: 9 --- cost = 19107.356854164245\n",
      "Epoch: 0 --- cost = 19618.244641875408\n",
      "Epoch: 9 --- cost = 19284.14159160386\n",
      "Epoch: 0 --- cost = 19703.895572566424\n",
      "Epoch: 9 --- cost = 19371.991344065576\n",
      "Epoch: 0 --- cost = 19426.523017473635\n",
      "Epoch: 9 --- cost = 19083.382980595135\n",
      "Epoch: 0 --- cost = 19597.6584684175\n",
      "Epoch: 9 --- cost = 19235.7863395003\n",
      "Epoch: 0 --- cost = 19702.695426544047\n",
      "Epoch: 9 --- cost = 19357.687897738342\n",
      "Epoch: 0 --- cost = 19419.800121779812\n",
      "Epoch: 9 --- cost = 19078.67897539546\n",
      "Epoch: 0 --- cost = 19597.228913639738\n",
      "Epoch: 9 --- cost = 19228.761710310813\n",
      "Epoch: 0 --- cost = 19757.669960638796\n",
      "Epoch: 9 --- cost = 19470.641274991827\n",
      "Epoch: 0 --- cost = 19480.098150953454\n",
      "Epoch: 9 --- cost = 19131.777608744032\n",
      "Epoch: 0 --- cost = 19586.69068512802\n",
      "Epoch: 9 --- cost = 19317.185292425977\n",
      "Epoch: 0 --- cost = 19747.968196138085\n",
      "Epoch: 9 --- cost = 19488.738927878447\n",
      "Epoch: 0 --- cost = 19467.863908029005\n",
      "Epoch: 9 --- cost = 19133.08154114449\n",
      "Epoch: 0 --- cost = 19585.653886801527\n",
      "Epoch: 9 --- cost = 19299.166825831933\n",
      "Epoch: 0 --- cost = 19721.087305025147\n",
      "Epoch: 9 --- cost = 19445.318195941818\n",
      "Epoch: 0 --- cost = 19429.713396440537\n",
      "Epoch: 9 --- cost = 19111.042305720697\n",
      "Epoch: 0 --- cost = 19576.483577927935\n",
      "Epoch: 9 --- cost = 19247.20775629199\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([categorify(s_train.iloc[i]) for i in range(s_train.shape[0])])\n",
    "\n",
    "modelRuns = {}\n",
    "\n",
    "# train bag of words against various embedding dimensions (sizes), window sizes, and learning rates\n",
    "for (s, w, l) in itertools.product(sizes, windows, learningRates):\n",
    "    wv = word2vec[(s,w)].wv\n",
    "    X, Y = prepXY(pT_train, s_train, wv)\n",
    "    modelRuns[(s,w,l)] = bagOfWordsmodel(X, Y, learning_rate = l, num_iterations = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LazyConfMat(Ylabels, Yhat):\n",
    "    # Input:\n",
    "    # Ylabels - (m,) true labels [0-2]\n",
    "    # Yhat - (m,) predicted labels [0-2]\n",
    "    # Returns \n",
    "    confMat = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            confMat[i,j] = sum((Ylabels[:] == j) * (Yhat[:] == i))\n",
    "            \n",
    "    return confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For size 10, window 2, learning rate 0.1 the conf mat is \n",
      "[[414. 582. 303.]\n",
      " [ 42. 180.  62.]\n",
      " [ 72. 164. 204.]]\n",
      "the accuracy is 0.3944636678200692 and recall for Negative sentiment is 0.7840909090909091.\n",
      "\n",
      "For size 10, window 2, learning rate 0.01 the conf mat is \n",
      "[[  1.   3.   0.]\n",
      " [416. 744. 360.]\n",
      " [111. 179. 209.]]\n",
      "the accuracy is 0.47157686604053384 and recall for Negative sentiment is 0.001893939393939394.\n",
      "\n",
      "For size 10, window 2, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [522. 909. 549.]\n",
      " [  6.  17.  20.]]\n",
      "the accuracy is 0.4592189817103312 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 10, window 5, learning rate 0.1 the conf mat is \n",
      "[[421. 571. 303.]\n",
      " [ 36. 184.  70.]\n",
      " [ 71. 171. 196.]]\n",
      "the accuracy is 0.39594661393969355 and recall for Negative sentiment is 0.7973484848484849.\n",
      "\n",
      "For size 10, window 5, learning rate 0.01 the conf mat is \n",
      "[[  2.   4.   0.]\n",
      " [414. 694. 321.]\n",
      " [112. 228. 248.]]\n",
      "the accuracy is 0.4666337123084528 and recall for Negative sentiment is 0.003787878787878788.\n",
      "\n",
      "For size 10, window 5, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [526. 906. 559.]\n",
      " [  2.  20.  10.]]\n",
      "the accuracy is 0.4527928818586258 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 10, window 7, learning rate 0.1 the conf mat is \n",
      "[[444. 610. 323.]\n",
      " [ 25. 161.  51.]\n",
      " [ 59. 155. 195.]]\n",
      "the accuracy is 0.3954522985664854 and recall for Negative sentiment is 0.8409090909090909.\n",
      "\n",
      "For size 10, window 7, learning rate 0.01 the conf mat is \n",
      "[[  6.  11.   0.]\n",
      " [402. 669. 304.]\n",
      " [120. 246. 265.]]\n",
      "the accuracy is 0.46465645081562035 and recall for Negative sentiment is 0.011363636363636364.\n",
      "\n",
      "For size 10, window 7, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [483. 813. 455.]\n",
      " [ 45. 113. 114.]]\n",
      "the accuracy is 0.45823035096391496 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 50, window 2, learning rate 0.1 the conf mat is \n",
      "[[439. 647. 354.]\n",
      " [ 28. 154.  58.]\n",
      " [ 61. 125. 157.]]\n",
      "the accuracy is 0.3707365299060801 and recall for Negative sentiment is 0.8314393939393939.\n",
      "\n",
      "For size 50, window 2, learning rate 0.01 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [429. 739. 384.]\n",
      " [ 99. 187. 185.]]\n",
      "the accuracy is 0.45674740484429066 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 50, window 2, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [528. 922. 567.]\n",
      " [  0.   4.   2.]]\n",
      "the accuracy is 0.45674740484429066 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 50, window 5, learning rate 0.1 the conf mat is \n",
      "[[462. 659. 362.]\n",
      " [  9. 115.  35.]\n",
      " [ 57. 152. 172.]]\n",
      "the accuracy is 0.370242214532872 and recall for Negative sentiment is 0.875.\n",
      "\n",
      "For size 50, window 5, learning rate 0.01 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [423. 699. 353.]\n",
      " [105. 227. 216.]]\n",
      "the accuracy is 0.4522985664854177 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 50, window 5, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [521. 902. 546.]\n",
      " [  7.  24.  23.]]\n",
      "the accuracy is 0.45724172021749876 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 50, window 7, learning rate 0.1 the conf mat is \n",
      "[[472. 660. 373.]\n",
      " [  9. 132.  37.]\n",
      " [ 47. 134. 159.]]\n",
      "the accuracy is 0.3771626297577855 and recall for Negative sentiment is 0.8939393939393939.\n",
      "\n",
      "For size 50, window 7, learning rate 0.01 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [416. 695. 363.]\n",
      " [112. 231. 206.]]\n",
      "the accuracy is 0.44537815126050423 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 50, window 7, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [507. 869. 515.]\n",
      " [ 21.  57.  54.]]\n",
      "the accuracy is 0.45625308947108256 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 100, window 2, learning rate 0.1 the conf mat is \n",
      "[[437. 656. 352.]\n",
      " [ 25. 120.  36.]\n",
      " [ 66. 150. 181.]]\n",
      "the accuracy is 0.3648047454275828 and recall for Negative sentiment is 0.8276515151515151.\n",
      "\n",
      "For size 100, window 2, learning rate 0.01 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [429. 732. 380.]\n",
      " [ 99. 194. 189.]]\n",
      "the accuracy is 0.4552644587246663 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 100, window 2, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [528. 926. 568.]\n",
      " [  0.   0.   1.]]\n",
      "the accuracy is 0.45823035096391496 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 100, window 5, learning rate 0.1 the conf mat is \n",
      "[[456. 645. 357.]\n",
      " [  7. 102.  32.]\n",
      " [ 65. 179. 180.]]\n",
      "the accuracy is 0.3648047454275828 and recall for Negative sentiment is 0.8636363636363636.\n",
      "\n",
      "For size 100, window 5, learning rate 0.01 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [430. 713. 384.]\n",
      " [ 98. 213. 185.]]\n",
      "the accuracy is 0.4438952051408799 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 100, window 5, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [528. 925. 568.]\n",
      " [  0.   1.   1.]]\n",
      "the accuracy is 0.45773603559070686 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 100, window 7, learning rate 0.1 the conf mat is \n",
      "[[457. 638. 360.]\n",
      " [  9. 124.  34.]\n",
      " [ 62. 164. 175.]]\n",
      "the accuracy is 0.3737024221453287 and recall for Negative sentiment is 0.865530303030303.\n",
      "\n",
      "For size 100, window 7, learning rate 0.01 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [419. 703. 366.]\n",
      " [109. 223. 203.]]\n",
      "the accuracy is 0.4478497281265447 and recall for Negative sentiment is 0.0.\n",
      "\n",
      "For size 100, window 7, learning rate 0.001 the conf mat is \n",
      "[[  0.   0.   0.]\n",
      " [517. 901. 542.]\n",
      " [ 11.  25.  27.]]\n",
      "the accuracy is 0.4587246663371231 and recall for Negative sentiment is 0.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate models to see performance\n",
    "\n",
    "for s,w,l in itertools.product(sizes,windows,learningRates):\n",
    "    wv = word2vec[(s,w)].wv\n",
    "    W, b = modelRuns[(s,w,l)]\n",
    "    cf,_,_ = evaluateNaiveModel(pT_test, s_test, wv, W, b)\n",
    "    print(\"For size {}, window {}, learning rate {} the conf mat is \".format(str(s),str(w),str(l)))\n",
    "    print(cf)\n",
    "    accuracy = np.sum(np.diagonal(cf))/np.sum(cf)\n",
    "    negRecall = cf[0,0]/np.sum(cf[:,0])\n",
    "    print(\"the accuracy is \"+ str(accuracy) + \" and recall for Negative sentiment is \" + str(negRecall) +\".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy stays < 50%. Recall for negative sentiment is all over the place. We could try to play with parameters , but this doesn't look fruitful.\n",
    "\n",
    "#### Findings\n",
    "\n",
    "This bag of words approach is essentially runs a linear regression on the average of the word embeddings in each text. The fact that it performs so poorly indicates that either our embedding quality is quite low or that the structure of the sentences is very important or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM Based Model\n",
    "\n",
    "We saw that the \"bag of words\" approach was insufficient, suggesting that we need to develop a model that captures not just the set of words in a text but the order of those words. \n",
    "\n",
    "To this end, we employ an LSTM (\"Long Short Term Memory\") based neural net model. LSTM based models evaluate sequential data and learn if and how much of the past is important to predict some value. In our case, the LSTM model will hopefully read the text left to right and continuously decide if and which of the content that it has read will be useful to deciding the sentiment of the text. Again, this is in contrast to the \"bag of words\" approach that looks at all words in the text at once tries to decide sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2indexMapBuilder(wv):\n",
    "    # Input:\n",
    "    # wv - Word2Vec object\n",
    "    # Returns:\n",
    "    # dictionary mapping range(len(wv.vocab)) -> vocab\n",
    "    \n",
    "    word2index = {w:i for i,w in enumerate(wv.vocab)}\n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddingLayer(wv, word2index):\n",
    "    # Input:\n",
    "    # wv - Word2Vec object\n",
    "    # word2index - dict word in vocab -> index \n",
    "    # Returns:\n",
    "    # Keras embedding layer\n",
    "    \n",
    "    # get dimension of embedding space\n",
    "    embDim = wv[random.sample(list(wv.vocab),1)].shape[1]\n",
    "    \n",
    "    # get vocablen + 1 \n",
    "    vocabLen1 = len(wv.vocab)\n",
    "    \n",
    "    # initialize embedding matrix\n",
    "    embMat = np.zeros((vocabLen1,embDim))\n",
    "    \n",
    "    # fill in embedding matrix with embedding from wv\n",
    "    for w,i in word2index.items():\n",
    "        embMat[i,:] = wv[w]\n",
    "    \n",
    "    # set trainable = True bc we have low confidence in the quality of our embeddings\n",
    "    embLayer = Embedding(output_dim=embDim, input_dim=vocabLen1, trainable = True)\n",
    "    embLayer.build((None,))\n",
    "    embLayer.set_weights([embMat])\n",
    "    \n",
    "    return embLayer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentLSTM(wv, word2index, maxLen):\n",
    "    # Input:\n",
    "    # wv - Word2Vec object\n",
    "    # word2index - dict word in vocab -> index \n",
    "    # maxLen - some documents are extremely large so we will only train on their first maxLen words\n",
    "    \n",
    "    # Input will be the word indices\n",
    "    wordIndices = Input(shape=(maxLen,),dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer from the trained embedding\n",
    "    embedding_layer = embeddingLayer(wv,word2index)\n",
    "    embeddings = embedding_layer(wordIndices)\n",
    "    \n",
    "    # 128 dim LSTM layer \n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    # Dropout layer\n",
    "    X = Dropout(.5)(X)\n",
    "    # Another 128 dim LSTM layer\n",
    "    X = LSTM(128)(X)\n",
    "    # Dropout layer\n",
    "    X = Dropout(.5)(X)\n",
    "    # Dense layer with softmax activation to learn the 3 dim classification\n",
    "    X = Dense(3, activation='softmax')(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation(activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=[wordIndices],outputs=[X])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepXYforLSTM(word2index,pT_train, pT_test, s_train, s_test, maxLen):\n",
    "    \n",
    "    m_train = len(pT_train)\n",
    "    m_test = len(pT_test)\n",
    "    \n",
    "    X_train = np.zeros((m_train, maxLen))\n",
    "    \n",
    "    X_test = np.zeros((m_test, maxLen))\n",
    "    \n",
    "    Y_train = convert_to_one_hot(s_train)\n",
    "    \n",
    "    Y_test = convert_to_one_hot(s_test)\n",
    "    \n",
    "    for i in range(m_train):\n",
    "        sentences_concat = []\n",
    "        for s in pT_train.iloc[i]:\n",
    "            sentences_concat += s\n",
    "        for j in range(maxLen):\n",
    "            try:\n",
    "                X_train[i,j] = word2index[sentences_concat[j]]\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    for i in range(m_test):\n",
    "        sentences_concat = []\n",
    "        for s in pT_test.iloc[i]:\n",
    "            sentences_concat += s\n",
    "        for j in range(maxLen):\n",
    "            try:\n",
    "                X_test[i,j] = word2index[sentences_concat[j]]\n",
    "            except:\n",
    "                pass    \n",
    "            \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfrailey/miniconda3/envs/mlpractice/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cfrailey/miniconda3/envs/mlpractice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/cfrailey/miniconda3/envs/mlpractice/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 18s 1ms/step - loss: 1.0455 - acc: 0.4629 - val_loss: 1.0168 - val_acc: 0.4964\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 16s 985us/step - loss: 0.9951 - acc: 0.5238 - val_loss: 0.9705 - val_acc: 0.5481\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 16s 980us/step - loss: 0.9007 - acc: 0.6387 - val_loss: 0.9635 - val_acc: 0.5607\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 16s 972us/step - loss: 0.8198 - acc: 0.7253 - val_loss: 0.9540 - val_acc: 0.5810\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 16s 958us/step - loss: 0.7655 - acc: 0.7824 - val_loss: 0.9722 - val_acc: 0.5607\n",
      "2023/2023 [==============================] - 0s 203us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 32s 2ms/step - loss: 1.0437 - acc: 0.4681 - val_loss: 1.0161 - val_acc: 0.5063\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 29s 2ms/step - loss: 0.9905 - acc: 0.5378 - val_loss: 0.9742 - val_acc: 0.5574\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 29s 2ms/step - loss: 0.9227 - acc: 0.6166 - val_loss: 0.9945 - val_acc: 0.5376\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 31s 2ms/step - loss: 0.8781 - acc: 0.6654 - val_loss: 0.9654 - val_acc: 0.5678\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 29s 2ms/step - loss: 0.8431 - acc: 0.7008 - val_loss: 0.9618 - val_acc: 0.5711\n",
      "2023/2023 [==============================] - 1s 370us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 19s 1ms/step - loss: 1.0402 - acc: 0.4690 - val_loss: 1.0127 - val_acc: 0.4937\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 17s 1ms/step - loss: 0.9953 - acc: 0.5240 - val_loss: 0.9734 - val_acc: 0.5365\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 17s 1ms/step - loss: 0.9107 - acc: 0.6254 - val_loss: 0.9611 - val_acc: 0.5667\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 16s 955us/step - loss: 0.8269 - acc: 0.7169 - val_loss: 0.9580 - val_acc: 0.5761\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 16s 954us/step - loss: 0.7679 - acc: 0.7793 - val_loss: 0.9475 - val_acc: 0.5859\n",
      "2023/2023 [==============================] - 0s 207us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 34s 2ms/step - loss: 1.0464 - acc: 0.4694 - val_loss: 1.0240 - val_acc: 0.4854\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 30s 2ms/step - loss: 1.0198 - acc: 0.4997 - val_loss: 1.0081 - val_acc: 0.5157\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 30s 2ms/step - loss: 1.0149 - acc: 0.5092 - val_loss: 1.0093 - val_acc: 0.5113\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 32s 2ms/step - loss: 0.9301 - acc: 0.6060 - val_loss: 0.9547 - val_acc: 0.5794\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 30s 2ms/step - loss: 0.8682 - acc: 0.6746 - val_loss: 0.9452 - val_acc: 0.5898\n",
      "2023/2023 [==============================] - 1s 372us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 19s 1ms/step - loss: 1.0417 - acc: 0.4682 - val_loss: 1.0042 - val_acc: 0.5069\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 17s 1ms/step - loss: 0.9851 - acc: 0.5301 - val_loss: 0.9852 - val_acc: 0.5211\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 17s 1ms/step - loss: 0.9122 - acc: 0.6223 - val_loss: 0.9437 - val_acc: 0.5826\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 17s 1ms/step - loss: 0.8214 - acc: 0.7244 - val_loss: 0.9372 - val_acc: 0.5964\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 17s 1ms/step - loss: 0.7607 - acc: 0.7880 - val_loss: 0.9355 - val_acc: 0.6030\n",
      "2023/2023 [==============================] - 0s 229us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 34s 2ms/step - loss: 1.0432 - acc: 0.4704 - val_loss: 1.0073 - val_acc: 0.5250\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 31s 2ms/step - loss: 1.0024 - acc: 0.5231 - val_loss: 0.9924 - val_acc: 0.5211\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 32s 2ms/step - loss: 0.9303 - acc: 0.6056 - val_loss: 0.9502 - val_acc: 0.5892\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 32s 2ms/step - loss: 0.8547 - acc: 0.6901 - val_loss: 0.9546 - val_acc: 0.5826\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 31s 2ms/step - loss: 0.8076 - acc: 0.7398 - val_loss: 0.9598 - val_acc: 0.5821\n",
      "2023/2023 [==============================] - 1s 403us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 1.0309 - acc: 0.4872 - val_loss: 0.9647 - val_acc: 0.5519\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.9083 - acc: 0.6258 - val_loss: 0.9311 - val_acc: 0.5975\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7853 - acc: 0.7605 - val_loss: 0.9372 - val_acc: 0.6052\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7282 - acc: 0.8220 - val_loss: 0.9600 - val_acc: 0.5805\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7012 - acc: 0.8494 - val_loss: 0.9375 - val_acc: 0.6008\n",
      "2023/2023 [==============================] - 1s 268us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 40s 2ms/step - loss: 1.0343 - acc: 0.4821 - val_loss: 0.9791 - val_acc: 0.5475\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 37s 2ms/step - loss: 0.9300 - acc: 0.6065 - val_loss: 0.9765 - val_acc: 0.5618\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 38s 2ms/step - loss: 0.8897 - acc: 0.6574 - val_loss: 1.0369 - val_acc: 0.5140\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 38s 2ms/step - loss: 0.8228 - acc: 0.7246 - val_loss: 0.9582 - val_acc: 0.5837\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 37s 2ms/step - loss: 0.7707 - acc: 0.7788 - val_loss: 0.9346 - val_acc: 0.6085\n",
      "2023/2023 [==============================] - 1s 473us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 25s 2ms/step - loss: 1.0359 - acc: 0.4715 - val_loss: 1.0020 - val_acc: 0.5135\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 22s 1ms/step - loss: 0.9370 - acc: 0.5908 - val_loss: 0.9448 - val_acc: 0.5821\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.8175 - acc: 0.7257 - val_loss: 0.9266 - val_acc: 0.6118\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7443 - acc: 0.8038 - val_loss: 0.9270 - val_acc: 0.6090\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7050 - acc: 0.8450 - val_loss: 0.9347 - val_acc: 0.6112\n",
      "2023/2023 [==============================] - 1s 264us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16385/16385 [==============================] - 42s 3ms/step - loss: 1.0401 - acc: 0.4741 - val_loss: 1.0169 - val_acc: 0.5085\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 38s 2ms/step - loss: 0.9704 - acc: 0.5609 - val_loss: 0.9664 - val_acc: 0.5640\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 38s 2ms/step - loss: 0.8586 - acc: 0.6884 - val_loss: 0.9271 - val_acc: 0.6139\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 59s 4ms/step - loss: 0.8100 - acc: 0.7382 - val_loss: 0.9150 - val_acc: 0.6260\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 37s 2ms/step - loss: 0.7611 - acc: 0.7881 - val_loss: 0.9042 - val_acc: 0.6365\n",
      "2023/2023 [==============================] - 1s 432us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 25s 2ms/step - loss: 1.0399 - acc: 0.4665 - val_loss: 1.0172 - val_acc: 0.4898\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.9452 - acc: 0.5850 - val_loss: 0.9493 - val_acc: 0.5651\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.8208 - acc: 0.7241 - val_loss: 0.9279 - val_acc: 0.6079\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7407 - acc: 0.8078 - val_loss: 0.9294 - val_acc: 0.6068\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 21s 1ms/step - loss: 0.7070 - acc: 0.8433 - val_loss: 0.9341 - val_acc: 0.6052\n",
      "2023/2023 [==============================] - 1s 268us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 40s 2ms/step - loss: 1.0405 - acc: 0.4754 - val_loss: 1.0213 - val_acc: 0.4898\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 36s 2ms/step - loss: 0.9765 - acc: 0.5499 - val_loss: 0.9714 - val_acc: 0.5590\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 37s 2ms/step - loss: 0.8692 - acc: 0.6750 - val_loss: 0.9207 - val_acc: 0.6205\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 37s 2ms/step - loss: 0.7984 - acc: 0.7510 - val_loss: 0.9264 - val_acc: 0.6172\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 36s 2ms/step - loss: 0.7674 - acc: 0.7832 - val_loss: 0.9386 - val_acc: 0.6052\n",
      "2023/2023 [==============================] - 1s 462us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 29s 2ms/step - loss: 1.0226 - acc: 0.4901 - val_loss: 0.9698 - val_acc: 0.5376\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 25s 2ms/step - loss: 0.8986 - acc: 0.6395 - val_loss: 0.9420 - val_acc: 0.5925\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.7806 - acc: 0.7663 - val_loss: 0.9363 - val_acc: 0.6052\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.7242 - acc: 0.8250 - val_loss: 0.9358 - val_acc: 0.6035\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.6996 - acc: 0.8510 - val_loss: 0.9460 - val_acc: 0.5991\n",
      "2023/2023 [==============================] - 1s 263us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 44s 3ms/step - loss: 1.0312 - acc: 0.4853 - val_loss: 0.9684 - val_acc: 0.5568\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 41s 2ms/step - loss: 0.9091 - acc: 0.6286 - val_loss: 0.9473 - val_acc: 0.5843\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 40s 2ms/step - loss: 0.8089 - acc: 0.7382 - val_loss: 0.9148 - val_acc: 0.6277\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 39s 2ms/step - loss: 0.7506 - acc: 0.7982 - val_loss: 0.9075 - val_acc: 0.6321\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 40s 2ms/step - loss: 0.7204 - acc: 0.8301 - val_loss: 0.9236 - val_acc: 0.6167\n",
      "2023/2023 [==============================] - 1s 495us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 29s 2ms/step - loss: 1.0315 - acc: 0.4789 - val_loss: 0.9899 - val_acc: 0.5338\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.9056 - acc: 0.6317 - val_loss: 0.9276 - val_acc: 0.6090\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 23s 1ms/step - loss: 0.7741 - acc: 0.7731 - val_loss: 0.9390 - val_acc: 0.5997\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.7191 - acc: 0.8302 - val_loss: 0.9458 - val_acc: 0.5936\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 25s 2ms/step - loss: 0.6938 - acc: 0.8569 - val_loss: 0.9502 - val_acc: 0.5865\n",
      "2023/2023 [==============================] - 1s 280us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 45s 3ms/step - loss: 1.0361 - acc: 0.4808 - val_loss: 0.9977 - val_acc: 0.5162\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 39s 2ms/step - loss: 0.9285 - acc: 0.6051 - val_loss: 0.9194 - val_acc: 0.6172\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 41s 3ms/step - loss: 0.7934 - acc: 0.7538 - val_loss: 0.8898 - val_acc: 0.6463\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 40s 2ms/step - loss: 0.7311 - acc: 0.8187 - val_loss: 0.9115 - val_acc: 0.6315\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 39s 2ms/step - loss: 0.7042 - acc: 0.8460 - val_loss: 0.9038 - val_acc: 0.6431\n",
      "2023/2023 [==============================] - 1s 461us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 30s 2ms/step - loss: 1.0280 - acc: 0.4825 - val_loss: 0.9692 - val_acc: 0.5459\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.8861 - acc: 0.6509 - val_loss: 0.9193 - val_acc: 0.6118\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.7666 - acc: 0.7813 - val_loss: 0.9426 - val_acc: 0.5942\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.7225 - acc: 0.8265 - val_loss: 0.9339 - val_acc: 0.6052\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 24s 1ms/step - loss: 0.6974 - acc: 0.8533 - val_loss: 0.9324 - val_acc: 0.6101\n",
      "2023/2023 [==============================] - 1s 280us/step\n",
      "Train on 16385 samples, validate on 1821 samples\n",
      "Epoch 1/5\n",
      "16385/16385 [==============================] - 46s 3ms/step - loss: 1.0410 - acc: 0.4759 - val_loss: 1.0110 - val_acc: 0.5080\n",
      "Epoch 2/5\n",
      "16385/16385 [==============================] - 39s 2ms/step - loss: 0.9388 - acc: 0.5951 - val_loss: 0.9315 - val_acc: 0.6090\n",
      "Epoch 3/5\n",
      "16385/16385 [==============================] - 40s 2ms/step - loss: 0.8166 - acc: 0.7287 - val_loss: 0.9049 - val_acc: 0.6332\n",
      "Epoch 4/5\n",
      "16385/16385 [==============================] - 41s 3ms/step - loss: 0.7438 - acc: 0.8051 - val_loss: 0.9097 - val_acc: 0.6359\n",
      "Epoch 5/5\n",
      "16385/16385 [==============================] - 39s 2ms/step - loss: 0.7151 - acc: 0.8344 - val_loss: 0.9150 - val_acc: 0.6244\n",
      "2023/2023 [==============================] - 1s 463us/step\n"
     ]
    }
   ],
   "source": [
    "# train models for different sizes, windows, and maxlens \n",
    "# evaluate against test data\n",
    "\n",
    "maxLens = [10,20]\n",
    "\n",
    "LSTMmodels = {}\n",
    "LSTMmodelPerf = {}\n",
    "\n",
    "for s,w,m in itertools.product(sizes, windows, maxLens):\n",
    "    wv = word2vec[(s,w)].wv\n",
    "    word2index = word2indexMapBuilder(wv)\n",
    "    \n",
    "    LSTMmodels[(s,w,m)] = sentimentLSTM(wv, word2index,m)\n",
    "    \n",
    "    LSTMmodels[(s,w,m)].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = prepXYforLSTM(word2index,pT_train, pT_test, s_train, s_test, m)\n",
    "    \n",
    "    # fit model\n",
    "    LSTMmodels[(s,w,m)].fit(X_train, Y_train, epochs = 5, batch_size = 32, shuffle=True, validation_split=.1)\n",
    "    \n",
    "    \n",
    "    _, acc = LSTMmodels[(s,w,m)].evaluate(X_test, Y_test)\n",
    "    \n",
    "    Yhat = np.argmax(LSTMmodels[(s,w,m)].predict(X_test),axis=1)\n",
    "    Ylabels = np.argmax(Y_test,axis=1)\n",
    "    \n",
    "    confMat = LazyConfMat(Ylabels, Yhat)\n",
    "    \n",
    "    LSTMmodelPerf[(s,w,m)] = {'accuracy':acc, 'confMat':confMat}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('obj/LSTMmodels.pkl', 'wb') as output:\n",
    "    pickle.dump(LSTMmodelPerf,output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('obj/LSTMmodelPerf.pkl', 'wb') as output:\n",
    "    pickle.dump(LSTMmodelPerf, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 4 or 5 epochs, learning rates of .01 and .1 start to overfit as seen by the validation accuracy beginning to dip. \n",
    "\n",
    "The model with size =  50, window = 5, and maxLen = 20 has highest validation accuracy (.646) and is therefore used. We assess it's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is 0.634206623826001 and recall for Negative sentiment is 0.4962121212121212.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cf = LSTMmodelPerf[(50,5,20)]['confMat']\n",
    "\n",
    "accuracy = np.sum(np.diagonal(cf))/np.sum(cf)\n",
    "negRecall = cf[0,0]/np.sum(cf[:,0])\n",
    "print(\"the accuracy is \"+ str(accuracy) + \" and recall for Negative sentiment is \" + str(negRecall) +\".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alter the model to be more sensitive to negative sentiment weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By upweighting the Negative sentiment signal by a factor of 1.05,\n",
      "    we increased negative recall to 0.4962121212121212 but accuracy decreases to 0.6317350469599604.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.1,\n",
      "    we increased negative recall to 0.5 but accuracy decreases to 0.6317350469599604.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.1500000000000001,\n",
      "    we increased negative recall to 0.5 but accuracy decreases to 0.6307464162135442.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.2000000000000002,\n",
      "    we increased negative recall to 0.5 but accuracy decreases to 0.629757785467128.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.25,\n",
      "    we increased negative recall to 0.5056818181818182 but accuracy decreases to 0.6292634700939199.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.3,\n",
      "    we increased negative recall to 0.5113636363636364 but accuracy decreases to 0.6292634700939199.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.35,\n",
      "    we increased negative recall to 0.5151515151515151 but accuracy decreases to 0.6282748393475037.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.4000000000000001,\n",
      "    we increased negative recall to 0.5189393939393939 but accuracy decreases to 0.6287691547207118.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.4500000000000002,\n",
      "    we increased negative recall to 0.5246212121212122 but accuracy decreases to 0.629757785467128.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.5,\n",
      "    we increased negative recall to 0.5303030303030303 but accuracy decreases to 0.629757785467128.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.55,\n",
      "    we increased negative recall to 0.5378787878787878 but accuracy decreases to 0.6302521008403361.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.6,\n",
      "    we increased negative recall to 0.5397727272727273 but accuracy decreases to 0.6292634700939199.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.6500000000000001,\n",
      "    we increased negative recall to 0.5454545454545454 but accuracy decreases to 0.6292634700939199.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.7000000000000002,\n",
      "    we increased negative recall to 0.5473484848484849 but accuracy decreases to 0.6282748393475037.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.75,\n",
      "    we increased negative recall to 0.5492424242424242 but accuracy decreases to 0.6258032624814631.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.8,\n",
      "    we increased negative recall to 0.5492424242424242 but accuracy decreases to 0.6243203163618388.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.85,\n",
      "    we increased negative recall to 0.5511363636363636 but accuracy decreases to 0.6238260009886307.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.9000000000000001,\n",
      "    we increased negative recall to 0.5549242424242424 but accuracy decreases to 0.6228373702422145.\n",
      "By upweighting the Negative sentiment signal by a factor of 1.9500000000000002,\n",
      "    we increased negative recall to 0.5587121212121212 but accuracy decreases to 0.6223430548690064.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.0,\n",
      "    we increased negative recall to 0.5700757575757576 but accuracy decreases to 0.6243203163618388.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.05,\n",
      "    we increased negative recall to 0.5700757575757576 but accuracy decreases to 0.6223430548690064.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.1,\n",
      "    we increased negative recall to 0.5738636363636364 but accuracy decreases to 0.6213544241225902.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.1500000000000004,\n",
      "    we increased negative recall to 0.5795454545454546 but accuracy decreases to 0.6213544241225902.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.2,\n",
      "    we increased negative recall to 0.5814393939393939 but accuracy decreases to 0.6193771626297578.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.25,\n",
      "    we increased negative recall to 0.5833333333333334 but accuracy decreases to 0.6188828472565496.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.3,\n",
      "    we increased negative recall to 0.5871212121212122 but accuracy decreases to 0.6183885318833415.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.35,\n",
      "    we increased negative recall to 0.5928030303030303 but accuracy decreases to 0.6169055857637172.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.4000000000000004,\n",
      "    we increased negative recall to 0.5946969696969697 but accuracy decreases to 0.6139396935244686.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.45,\n",
      "    we increased negative recall to 0.6041666666666666 but accuracy decreases to 0.6119624320316361.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.5,\n",
      "    we increased negative recall to 0.6136363636363636 but accuracy decreases to 0.6104794859120118.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.55,\n",
      "    we increased negative recall to 0.6212121212121212 but accuracy decreases to 0.6085022244191794.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.6,\n",
      "    we increased negative recall to 0.6287878787878788 but accuracy decreases to 0.6030647553138903.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.6500000000000004,\n",
      "    we increased negative recall to 0.6458333333333334 but accuracy decreases to 0.5941670785961444.\n",
      "By upweighting the Negative sentiment signal by a factor of 2.7,\n",
      "    we increased negative recall to 0.6723484848484849 but accuracy decreases to 0.5793376173999011.\n"
     ]
    }
   ],
   "source": [
    "model = LSTMmodels[(50,5,20)]\n",
    "negWeights = 1.05+ np.arange(34)*.05\n",
    "accNegRecall = pd.DataFrame(columns=['accuracy','negativeRecall'])\n",
    "i = 0\n",
    "\n",
    "for w in negWeights:\n",
    "    weights = np.atleast_2d([w,1,1])\n",
    "    \n",
    "    Yhat = np.argmax(model.predict(X_test)*weights, axis = 1)\n",
    "    Ylabels = np.argmax(Y_test,axis=1)\n",
    "\n",
    "    cf = LazyConfMat(Ylabels, Yhat)\n",
    "    \n",
    "    accuracy = np.sum(np.diagonal(cf))/np.sum(cf)\n",
    "    negRecall = cf[0,0]/np.sum(cf[:,0])\n",
    "    \n",
    "    accNegRecall.loc[i]=[accuracy, negRecall]\n",
    "    i+=1\n",
    "    \n",
    "    print(\"\"\"By upweighting the Negative sentiment signal by a factor of {},\n",
    "    we increased negative recall to {} but accuracy decreases to {}.\"\"\".format(w,negRecall,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "The LSTM based model learns much better than the \"bag of words\" model and can be adjusted to increase sensitivity to negative sentiments if desired. We plot this tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1d6a486da0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x='accuracy',y='negativeRecall',data=accNegRecall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "421px",
    "top": "177px",
    "width": "256.76px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
