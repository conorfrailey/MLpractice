{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Sentiment Analysis\n",
    "\n",
    "__BACKGROUND__\n",
    "\n",
    "A large multinational corporation is seeking to automatically identify the sentiment that their customer base talks about on social media. They would like to expand this capability into multiple languages. Many 3rd party tools exist for sentiment analysis, however, they need help with under-resourced languages.\n",
    "\n",
    "__GOAL__\n",
    "\n",
    "Train a sentiment classifier (Positive, Negative, Neutral) on a corpus of the provided documents. Your goal is to maximize accuracy. There is special interest in being able to accurately detect negative sentiment. The training data includes documents from a wide variety of sources, not merely social media, and some of it may be inconsistently labeled. Please describe the business outcomes in your work sample including how data limitations impact your results and how these limitations could be addressed in a larger project.\n",
    "\n",
    "__DATA__ \n",
    "\n",
    "Link to data: http://archive.ics.uci.edu/ml/datasets/Roman+Urdu+Data+Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Inspection\n",
    "\n",
    "We first explore the data. We have been warned that the training data \"includes documents from a wide variety of sources, not merely social media, and some of it may be inconsistently labeled\" and we want to understand the implications of these challenges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random as random\n",
    "random.seed(5) \n",
    "import os\n",
    "import keras as k\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mpandas_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   2010\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m         \u001b[0mnpdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cde07657e129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data = pd.read_csv('RomanUrduDataSet.csv',sep=',',names=['text','sentiment'],index_col=False, \n\u001b[0;32m----> 2\u001b[0;31m                   quotechar='\"',na_values='NULL',dtype = ['str','str'])\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlpractice/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mpandas_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \u001b[0;31m# we don't want to force a repr of the non-string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2015\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data type not understood\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2016\u001b[0m         raise TypeError(\"data type '{}' not understood\".format(\n\u001b[1;32m   2017\u001b[0m             dtype))\n",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('RomanUrduDataSet.csv',sep=',',names=['text','sentiment'],index_col=False, \n",
    "                  quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sai kha ya her kisi kay bus ki bat nhi hai lak...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sahi bt h</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kya bt hai,</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wah je wah</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Sai kha ya her kisi kay bus ki bat nhi hai lak...  Positive\n",
       "1                                          sahi bt h  Positive\n",
       "2                                        Kya bt hai,  Positive\n",
       "3                                         Wah je wah  Positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20229, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20228</td>\n",
       "      <td>20229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19664</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Good</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>23</td>\n",
       "      <td>8929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text sentiment\n",
       "count   20228     20229\n",
       "unique  19664         4\n",
       "top      Good   Neutral\n",
       "freq       23      8929"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     8929\n",
       "Positive    6013\n",
       "Negative    5286\n",
       "Neative        1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there is a mislabelling ('Neative' should be 'Negative' I believe). We correct that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['sentiment'] == 'Neative', 'sentiment'] = 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     8929\n",
       "Positive    6013\n",
       "Negative    5287\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20229 entries, 0 to 20228\n",
      "Data columns (total 2 columns):\n",
      "text         20228 non-null object\n",
      "sentiment    20229 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 316.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>Hahahah sanam yad ha?</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>Wo gaeyki kea mal ko bandage bana dete hain</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6141</th>\n",
       "      <td>Promotion main k creative main</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11072</th>\n",
       "      <td>Liaquatabad Me Firing 1 Shaks Zakhmi</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13902</th>\n",
       "      <td>Ditto copy hai batay bhi apki trah krta hai</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12383</th>\n",
       "      <td>Rajesh Khanna apne urooj ke dino mien jab bah...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8551</th>\n",
       "      <td>Inka aik beata beh tha agar uski pic dekha day...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19951</th>\n",
       "      <td>Hamari aadhi se zayada awaam toh andhi hai apn...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>mere sheer🦁 mere cheety🐯 😘😘😘</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>Tujhe pata ha me q Hans Raha ho ye jis k bat K...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8293</th>\n",
       "      <td>Raza you will lose your seat this time. First ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>Usy Libaas Mazhab</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>Yeh one day international cricket ka sab se ba...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11551</th>\n",
       "      <td>PTI Ne Phir Ayaz Sadiq Ki Kamyabi Challenge Ka...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14683</th>\n",
       "      <td>Jia arsal dono k parents b boht funny hai yr</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>dikhaty kuch hain or bhejty kuch or</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10526</th>\n",
       "      <td>Ae yaad feroza ha tag tum maham ko karlo .. ye...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>Agree band hona chahiay</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10180</th>\n",
       "      <td>Yess I knw sahi yaad dilaya haha per safai k l...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>Jitna zoor laga loo result 0 hi ho ga tum loog...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18711</th>\n",
       "      <td>Sab say ziyada maza to pichay walo ko aarha hai</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>Theek h phir meri traf sy haan smjhiye</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4552</th>\n",
       "      <td>Aap khud mil k kardain shukriya 😛</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12795</th>\n",
       "      <td>Hafsa my gf told me that me just cn4m kar raha...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10936</th>\n",
       "      <td>Par 6 9 Shiddat Ka Zalzala</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14702</th>\n",
       "      <td>hum tv is ka end na karan</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>swing ke sultan ko duniya ke azeem tareen bowl...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16892</th>\n",
       "      <td>bilkul hat key....kamal</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17704</th>\n",
       "      <td>poora season aik din me yes.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9029</th>\n",
       "      <td>zbrdast beta</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14824</th>\n",
       "      <td>Nhii sherry kesath bhi nhi hogi kinza ki shadi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>BALUCHISTAN K Maujoda Governor Ka Name Mohamma...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>Kyun ke main ne yeh iradah kiya hai ke 2014 mi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Lady Diana das (10) se zaid bare aur ban ul aq...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>Kha marey juthi ksm 😒</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10396</th>\n",
       "      <td>Bht shouq hai tujhe phasne ka ,, Aik bar agai ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>haha</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>Es shampoo say bal sarah dil naram rhte ha. Th...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>Faiz Ahmed Faiz 13 February 1911 ko Sialkot ke...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18392</th>\n",
       "      <td>this video was awesome. but vote to Imran Khan...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>Qeyam-e-pakistan ke bad aap Lahore aa gae aur ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8647</th>\n",
       "      <td>Sari larkia Ya tu makeup ki dukan Ya gudammmm</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9526</th>\n",
       "      <td>Us waqt ka socho jab Karachi final mai pindi b...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17777</th>\n",
       "      <td>Abhi inko sahe .......</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>iss budhii sy bnda puchay kitnii jamatain pass...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>Okay 😂😂😂😂😂😂</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17813</th>\n",
       "      <td>Nzr andaz krna achi bat waisy aisi postain</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17019</th>\n",
       "      <td>Ya lanti jinho na fasla kia ha in bgrto ka pat...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11178</th>\n",
       "      <td>Motor Cycle Sawar Ehtyat Karen</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13771</th>\n",
       "      <td>Isi se miltq julta tha ab kiya me tmhare wo na...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>Durre Shahwar</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7226</th>\n",
       "      <td>Un ka phela boy friend 14 baras ki umar mein b...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12998</th>\n",
       "      <td>Kutee ki bachi</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>NA 122 election Ballot Papers tyar ECP kay haw...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>aik ubqari ka amal hai ya rabee mousa ya rabe ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7713</th>\n",
       "      <td>Ye Pakistan ki phli rangeen film thi</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>Iss kay baad alla taleem kay liye</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>Is tarha District Court mein apni practice jar...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11949</th>\n",
       "      <td>Condoleezza Rice par is hawale se bhi tanqeed...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15454</th>\n",
       "      <td>Salam ap bilkol online nhee aty</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "4360                               Hahahah sanam yad ha?   Neutral\n",
       "7269         Wo gaeyki kea mal ko bandage bana dete hain   Neutral\n",
       "6141                      Promotion main k creative main   Neutral\n",
       "11072              Liaquatabad Me Firing 1 Shaks Zakhmi   Negative\n",
       "13902       Ditto copy hai batay bhi apki trah krta hai    Neutral\n",
       "12383   Rajesh Khanna apne urooj ke dino mien jab bah...  Negative\n",
       "8551   Inka aik beata beh tha agar uski pic dekha day...   Neutral\n",
       "19951  Hamari aadhi se zayada awaam toh andhi hai apn...  Negative\n",
       "4786                        mere sheer🦁 mere cheety🐯 😘😘😘   Neutral\n",
       "3737   Tujhe pata ha me q Hans Raha ho ye jis k bat K...  Positive\n",
       "8293   Raza you will lose your seat this time. First ...   Neutral\n",
       "5578                                   Usy Libaas Mazhab   Neutral\n",
       "2714   Yeh one day international cricket ka sab se ba...  Positive\n",
       "11551  PTI Ne Phir Ayaz Sadiq Ki Kamyabi Challenge Ka...  Negative\n",
       "14683       Jia arsal dono k parents b boht funny hai yr  Positive\n",
       "11819                dikhaty kuch hain or bhejty kuch or  Negative\n",
       "10526  Ae yaad feroza ha tag tum maham ko karlo .. ye...  Negative\n",
       "5211                             Agree band hona chahiay   Neutral\n",
       "10180  Yess I knw sahi yaad dilaya haha per safai k l...  Negative\n",
       "2158   Jitna zoor laga loo result 0 hi ho ga tum loog...  Positive\n",
       "18711    Sab say ziyada maza to pichay walo ko aarha hai  Positive\n",
       "16668             Theek h phir meri traf sy haan smjhiye  Positive\n",
       "4552                   Aap khud mil k kardain shukriya 😛   Neutral\n",
       "12795  Hafsa my gf told me that me just cn4m kar raha...  Negative\n",
       "10936                        Par 6 9 Shiddat Ka Zalzala   Negative\n",
       "14702                         hum tv is ka end na karan    Neutral\n",
       "1446   swing ke sultan ko duniya ke azeem tareen bowl...  Positive\n",
       "16892                            bilkul hat key....kamal  Positive\n",
       "17704                       poora season aik din me yes.   Neutral\n",
       "9029                                        zbrdast beta   Neutral\n",
       "14824  Nhii sherry kesath bhi nhi hogi kinza ki shadi...   Neutral\n",
       "5519   BALUCHISTAN K Maujoda Governor Ka Name Mohamma...   Neutral\n",
       "2204   Kyun ke main ne yeh iradah kiya hai ke 2014 mi...  Positive\n",
       "1160   Lady Diana das (10) se zaid bare aur ban ul aq...  Positive\n",
       "4744                               Kha marey juthi ksm 😒   Neutral\n",
       "10396  Bht shouq hai tujhe phasne ka ,, Aik bar agai ...  Negative\n",
       "8623                                                haha   Neutral\n",
       "1194   Es shampoo say bal sarah dil naram rhte ha. Th...  Positive\n",
       "6650   Faiz Ahmed Faiz 13 February 1911 ko Sialkot ke...   Neutral\n",
       "18392  this video was awesome. but vote to Imran Khan...  Positive\n",
       "2954   Qeyam-e-pakistan ke bad aap Lahore aa gae aur ...  Positive\n",
       "8647       Sari larkia Ya tu makeup ki dukan Ya gudammmm   Neutral\n",
       "9526   Us waqt ka socho jab Karachi final mai pindi b...   Neutral\n",
       "17777                            Abhi inko sahe .......    Neutral\n",
       "20143  iss budhii sy bnda puchay kitnii jamatain pass...  Negative\n",
       "4850                                         Okay 😂😂😂😂😂😂   Neutral\n",
       "17813         Nzr andaz krna achi bat waisy aisi postain   Neutral\n",
       "17019  Ya lanti jinho na fasla kia ha in bgrto ka pat...  Negative\n",
       "11178                    Motor Cycle Sawar Ehtyat Karen   Negative\n",
       "13771  Isi se miltq julta tha ab kiya me tmhare wo na...  Negative\n",
       "5106                                       Durre Shahwar   Neutral\n",
       "7226   Un ka phela boy friend 14 baras ki umar mein b...   Neutral\n",
       "12998                                     Kutee ki bachi  Negative\n",
       "5484   NA 122 election Ballot Papers tyar ECP kay haw...   Neutral\n",
       "16362  aik ubqari ka amal hai ya rabee mousa ya rabe ...   Neutral\n",
       "7713                Ye Pakistan ki phli rangeen film thi   Neutral\n",
       "1864                  Iss kay baad alla taleem kay liye   Positive\n",
       "1131   Is tarha District Court mein apni practice jar...  Positive\n",
       "11949   Condoleezza Rice par is hawale se bhi tanqeed...  Negative\n",
       "15454                    Salam ap bilkol online nhee aty   Neutral"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data a bit more\n",
    "data.sample(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's one empty 'text' value \n",
    "data['text'] = data['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLength'] = data['text'].apply(len)\n",
    "data['textWordCount'] = data['text'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>textLength</th>\n",
       "      <th>textWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7949</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8282</th>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text sentiment  textLength  textWordCount\n",
       "7949        Neutral           1              2\n",
       "7951        Neutral           1              2\n",
       "8028        Neutral           1              2\n",
       "8073        Neutral           1              2\n",
       "8074        Neutral           1              2\n",
       "8079        Neutral           1              2\n",
       "8239        Neutral           1              2\n",
       "8282        Neutral           1              2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['text']==' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20229 entries, 0 to 20228\n",
      "Data columns (total 4 columns):\n",
      "text             20229 non-null object\n",
      "sentiment        20229 non-null object\n",
      "textLength       20229 non-null int64\n",
      "textWordCount    20229 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 632.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f292cb090f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEMCAYAAABnWmXlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAZK0lEQVR4nO3deYwe913H8ffMc6937X12vXa2wXYc2/klzkmaOAcJaUlJr5SWo4WqUBA0SiugHOISQggQ5RICqaWIgNrSE4mqFJI2aQ6atglK27QhLbl+cdL4dhLHu+vs7nPPDH/Ms+v1evfZ65nfYz/P5yVZsZ95dubnyeiTX76/Y7woihARETf8TjdARKSXKHRFRBxS6IqIOKTQFRFxSKErIuJQusWxHHA1cBQI3DRHROSslwJGgUeB6vyDrUL3auChhBolItLtbgQenv9hq9A9CjA+Pk0Yrm0u7/BwP8ePT63pHL1A92l5dJ+WR/dpedp9n3zfo1hcB80Mna9V6AYAYRitOXRnziNL031aHt2n5dF9Wp6E7tOCZVkNpImIOKTQFRFxSKErIuKQQldExCGFroiIQwpdERGHFLoiIg4pdEVEHFLoiog4pNAVEXFIoSsi4pBCV0TEIYWuiIhDCl0REYcUuiIiDil0RUQcUuiKiDik0BURcch56D755P/xzW/+j+vLioicEVq9Iy0R999/D3v3PsuePdfh++poi0hvcZ56pXKJcrnEwYP7XV9aRKTjnIduuVQG4Omnn3R9aRGRjutAT3cmdJ9yfWkRkY5zHrqVShy61j5No9FwfXkRkY5yGrpRFFGtlPEy/dRqVfbvf8Hl5UVEOs5p6NbrdcIwJD1wLgDPPKMSg4j0FqehW27Wc/1MP6n8oAbTRKTnOA3dSqUEgJfK4Bc28exeS71ed9kEEZGOchy6leZV06TWbaJRr/PCC8+7bIKISEd1pLzg+RnSfZsA1XVFpLd0LHS9VJZUYYinnnrCZRNERDrKcXmhGbqpTHzx/BAHDmg5sIj0jo70dPHj0PXS66hUyidrvSIiXa5D5YV4czM/UwBgfHzMZTNERDrG/ZQxzwcvBYCX7gNgbOy4y2aIiHSM8yljfiqD53nxxdXTFZEe47y84DXruXCyp6vQFZFe4b6m6598WYXnp0hl8gpdEekZ7kPXm/eGoFRBNV0R6RmOQ7c0O11sVrrA8eMKXRHpDU5Dt1QuzS6MmG1Auo8xlRdEpEe4nb1QLs/O0Z3hZfool6apVqsumyIi0hFuQ7daPa28MDNtbGJCvV0R6X7OQjcMQ+q16ilTxmDuAgmFroh0P2ehO7vZzWk9Xa1KE5He4Sx0Zze7Sc2r6aa1Kk1Eeofz0D2tvOCn8dM5ha6I9ISOlxcgrusqdEWkF3S8pwtogYSI9AznPV3mzdONPypwXANpItID3Pd0UwuUFzJ9lKanqNdrrpojItIRZ0RN15/d4nHcVXNERDrC/ZSxBcoLnjYzF5Ee4TR0PT+N551+SV+v7RGRHuG0vOCnsgsem+npaimwiHQ75z3dhXh+Bj+dZXxcPV0R6W5Oe7rRQnN0m7RAQkR6gduBtOar1xeU0gIJEel+zkK3VCotvBptpiEZvStNRLqf257uAgsjZniZAaamJpmennLVJBER55zWdFv1dFP5QQAOHTroqkkiIs45Cd0oiqhUK63LC83QPXBgn4smiYh0hJPQrVarRGF4+uvX5zYkXSCVKXDgwH4XTRIR6QgnoXtys5uF5+nOym5Q6IpIV3Mbui16ugCpfJEjRw7TaDRcNEtExDknoVsqlYClQ9fPDRIEDY4ePeKiWSIizjkN3YV2GJtrZjDt4EGVGESkO51ZPd3sAJ6fVl1XRLqW29BtsTgCwPN8/NwGTRsTka7ldCCt1ZSxGX5uA/sP7CeKooRbJSLi3hlVXgDwc0XKpWntOCYiXcldT9fz419LSOWLAKrrikhXctbT9VMZPM9b8rt+bgOgGQwi0p2che5ySgsQD7alcgMKXRHpSg5Dd4klwHN42UH27d+XXINERDrEWehG3vJD188P8sqxlxkfH0+wVSIi7jkJ3bGxcbx0ftnfz6zfCsBDDz2YVJNERDrCUeiO4aULy/6+nx0gtW6UBx/8b4IgSLBlIiJuJR661WqFcrm0otAFyBZ3cuLEOI8//lhCLRMRcS/x0J2py/qZlYVuqn+UVHYdX/3q/Uk0S0SkIxyEbryybKU9Xc/zSW04n6effoIXXzyaRNNERJxLPHQnJuKerpfuW/HPZgZ34Hk+X/vaA+1ulohIRzgLXX+FPd34Z/KkBn6Ibzz0darVSrubJiLinJPygp/KLLmt42IyxV1UyiUefvgbbW6ZiIh7TgbS/MzKSwszUoWNpArDfOXeLxOGYRtbJiLinpOebuQvf2HEfJ7nkRm6kOOvHOOxxx5tY8tERNxLPHTHxsfwVjhdbL70wLmkcgPcffdd2txcRM5qiYZuGIa8emJiVYNoc3meT7p4Afv2/YC9e22bWici4l6ioTs5OUkYhiueo7uQzIbt+Ok899zzpTa0TESkMxIN3YmJmYURqx9Im+H5adKDO/je9x7j8OFDaz6fiEgnJBq6M6vRVroEeDGZ4gV4fpp77rmrLecTEXEt4Z7uBLDyJcCL8dM50oM7eOSR/+HYsZfbck4REZcc9HS9Fe2lu5TskAFQb1dEzkoJh+44qUwebxlvAV4uP9NHesN2Hnro67NLjEVEzhbJD6S1qbQwV3b4IoIg4N577277uUVEkpRo6B4fG8NLtT90/Ww/6fVbefDBB3j11RNtP7+ISFIS7umOt20Qbb7sxt3UGw0+8pG/p16vJXINEZF2Syx0a7Ua5dL0mpcALyaV20Bu9Bqef/5ZPvaxO7QZjoicFRIL3bXso7tcmfVbyY5czre//Qhf/OLnE7uOiEi7pJM68cnX9Kx9NVor2eELieqTfPnL/8Xhwwd54xvfygUXXIjneYleV0RkNRIM3eZrehIqL8zwPI/cOVfhpfv4/hNP8vjjj7F163nccMNNXHnlVQwNDSd6fRGRlUgsdGf2XUiyvDDD83xyI5eQHb6Q+ol9HH55L5/73Cf53Oc+ybZt27n88h/mkksuY/v2HaRSqcTbIyKymARDdxzPT4O/utf0rIbnp8kWd5It7iSsvkp98hCHXj7M/jv/gzvv/A/y+QKXX34Fe/ZczyWXXEYm465tIiKQcHnBz/R1rLbq59aTy+2GjbuJgiqN6ZcIpl7k0e8+xre+9Qj5fIGdO3exceMIGzeOsGnTZjZtOofNmzeTy7Vv2bKIyFzJDqSlTg2vKIqojz9HY+oIRA2ioE4U1vH8BV5c6aVJD7yGzODONQe3l8qRWb+VzPqtRFFIMP0SjVcP8PTzh4iesYSN6infHxhYz/DwRoaHNzI4WKS/v5/+/n76+taRzxcoFArkcjlSqRS+nyKTSVMo9FEo9Kn3LCItJRa6U1NTeKks9YkXqJ/4AQBhvUxUn5r9Tj6f55Y338J9991HpTRx2jmC6aPUjtsVbQ2Z2XA+mcHtix73PJ90/yjp/tHZz6KgTlifIqxNEtYmqdSmOXhsmoNHjxM1yoSBFl+I9Jqbb76F97znl9p+3sRCd0Fh/ZQ/3nLLLdx2221EUcRddy2ya1hYBxKeAZHK4HsDgAd4RF4K/BSRlwY/TSqoEtSmljqNiHSRo0ePEEVR20ukiYVuoVAgmpwiM7h9tudZG99L9cXvzn7nvvvuI4oi7r///kXPkx25lGxxZ1vbFtamaEweJKhMENWnISgR1EqnfCeXyzM0NEyxOEp//8BseaFQKJDPnywvpFIp0umT5YV8Pk86ncb3fXzfx/M8PM/H9z0ymSzpdHrRf4kjIwMcOzbZ1r9rN9J9Wh7dp+VxfZ8SC92hoSEOHH3llM8yg3F4Nibjmm4tqPOlr3wVz8+T6hs49QSzNd0da25LFEWE1RME0y/GYVs+DsBgcYhN52xiZCT+dc45o2zePMrIyCb6+pJd1CEivSmx0B0cLBLVy6d85nke2eIussVdSV12VhQ2aEy/SGPyEGHpJcJmW7Zs2cY11/w4V199LSMjmxJvh4jIXAmG7hBhUCMKG/F8XQeiKCIoHaM+vpdg+ihR2CBf6OPSKy7nkksu4+KLL9UKNRHpqMTSsFgsAhDVS3i59UldJr5GFNGYPEh97BmC8hh9feu45qbXc+WVV2HMRaTTbscLRUQWk2DoDgEQNsr4CYdubewZai9/j02bz+FN7/wVrr/+RrLZbKLXFBFZjURrugBRo7zEN9em/uoBai9/j6uvvpbbb/81fD/RfdlFRNYksYSaCd0wwdANSq9QPfotduy4gPe97/0KXBE54yWWUvFS2fxpMxjaJayXqRx+mI3Dw3zwg79NJqNygoic+RLtGg4OFokapaW/uAq1409DWOM3f/N3GRhItmYsItIuiYbu0NBQIjXdsFGhceIHXHfdDYyOntv284uIJCXR0C0WhyCotP289bFnicKAt7zlJ9p+bhGRJCVeXgjqZaIoats5o6BGY2IvV121h9HR17TtvCIiLiTc0y1CFBIF1aW/vEy18b2EQZ23vvXtbTuniIgrCfd04wUSUb09g2lR2KAxvpdLL72cbdvOa8s5RURcSr6nS/sWSNQnfkDYqHDrre9oy/lERFxLfiCN9iyQiKKQxviznL9jF7t2mTWfT0SkExIN3fXrN+B5XlvKC43JQwS1Kd7y5lvb0DIRkc5INHRTqRT9A+vXXF6Iooj62DOMbNrMFVe8tk2tExFxL/HNCoaKQ2suLwSlYwTlMd78plu1v4KInNWSD92hIbw1LpCojz3Duv4Brr/+xja1SkSkMxIP3cHBtfV0g+oJGlNH+PE3vFF75IrIWS/x0C0Wi4SNKlHYWNXP18f2kk5neP3r39DmlomIuOegp7v6ubpRUCeY3M8111ynncREpCs46Omufq5u/dV9hEFdvVwR6Rrueror3Mw8iiIaE8+zdet5bN++I4mmiYg456ynu9LyQlA+RlCZ4Oabb8HzvCSaJiLiXOKhWygUyOZyhCt8g0R9/DnyhT727LkuoZaJiLiXeOh6nkexOLSi8kLYqBBMHuLGG24il8sl2DoREbecLO/aODxEtIIFEo1X9xNFIa973c0JtkpExD0nobtu3TpYwTzdoDzGhsGi3gwhIl3HSej29fVBtPzQjWonOE+blItIF3ISuoVCAcL6sr4bhQFB9QRbtmxLuFUiIu456+mGwfJCN6yegChi61aFroh0H2c93SgMiMJgye+G1QkA9XRFpCu5q+nCsja9CSrjZLM5RkY2Jd0sERHnnIbucuq6YXWCLVu2abNyEelKjnu6rUM3iiKi6oTquSLStc6s0K1PEwZ1ha6IdC13U8YAgtY13aASD6IpdEWkW51RPd2wOo7neZx77hYXzRIRcc5pT3fJ0K1MsGnzqN6FJiJd64yavRDVJrT8V0S6mpPQzefz4HlELValRUGNoDbNli1bXTRJRKQjnISu53nkc/mWiyOCyjigQTQR6W7OViDk84WWNd2Ty3/Pc9QiERH3nIXuUjuNhbVJCoU+NmzY4KpJIiLOOQ3dlj3deomhoWFXzRER6QjHPd0WiyMaZYaHFboi0t3chm6rt0cEZfV0RaTrORxI61u0phuFAUG9QrE45Ko5IiId4bamu8g83agRv55doSsi3c5p6IZBnSiKTjsW1kuAQldEup/TebrAgoNpUSMOXdV0RaTbuR1IY+FNb8K6ygsi0hsc9nTzwMKhGzVK5POF2e+IiHQr5z3dBcsL9RJFlRZEpAc4r+kuNIMhCsoMD6m0ICLdz2FPt8XbIxpaGCEivaED5YVTQzeKAoK6QldEeoP78sL80NXMBRHpIR2YvXDqQFqo1Wgi0kOchW46nSadzpw2kBZpNZqI9BBnoQvN3u788oJWo4lID3EcuqdvZB7Wy+Ry+ZMDbSIiXcxp6Bb6+k6r6UaNkkoLItIznIZuX6EA82u6jZLeGCEiPcNtT7fQd/rbIxoVikWFroj0BsehW4DoZE83ikKCeokhLQEWkR7hfvZCcLKnO/PGCM1cEJFe4Xz2QhjUZv+s1Wgi0muclxeiKCQKAwDChhZGiEhvcV/T5eT+CzOr0VTTFZFe4by8AMyuSgsbJbLZ3Oy2jyIi3a5DPd14MC2sjDM6ei6e57lshohIx7ifp0v89ogobBCWj7N798UumyAi0lHup4wBhHWC0itEUchFF+122QQRkY5Ku7zY3I3Mw+or+L7Pzp3GZRNERDqqYzXdoPQy27fv0GvXRaSndCZ06yWCyhgXXaR6roj0Fqehm83m8DyPxtQRiCKFroj0HKeh63keuVyesDpBKpVmx45dLi8vItJxTkMXIN8sMezYuYtsNuv68iIiHeU8dAvNGQy7VVoQkR7kPnT74gUSF16o+bki0nuch25foUAmk+H883e6vrSISMc5XRwBsGfPdezceQHptPNLi4h0nPPku+GGm1xfUkTkjOG8vCAi0ssUuiIiDil0RUQcUuiKiDik0BURcUihKyLikEJXRMQhha6IiEMKXRERhxS6IiIOKXRFRBxS6IqIOKTQFRFxSKErIuKQQldExCGFroiIQwpdERGHFLoiIg61el1PCsD3vbZcqF3n6Xa6T8uj+7Q8uk/L0877NOdcqYWOe1EULfazNwAPta0lIiK95Ubg4fkftgrdHHA1cBQIkmuXiEhXSQGjwKNAdf7BVqErIiJtpoE0ERGHFLoiIg4pdEVEHFLoiog4pNAVEXFIoSsi4pBCV0TEoVbLgNfMGHMB8ElgGDgOvNdauzfJa57JjDH7gErzF8DvW2vvNcZcC9wBFIB9wM9ba19u/syix7qBMeZvgZ8GzgMutdY+0fx80WdntcfOdi3u1T4WeK6ax3rq2TLGDAOfBnYQL0x4DrjdWntstfei3fcp6Z7uPwEftdZeAHyUuOG97mestVc0f91rjPGAzwC/2rxP3wD+CqDVsS7yn8CPAvvnfd7q2VntsbPdYvcK5j1X0Pr56eJnKwL+xlprrLWXAc8Df7Xae5HEfUosdI0xm4ArgX9rfvRvwJXGmJGkrnmWugqoWGtn1mj/E/CuZRzrCtbah621B+d+1urZWe2xpP8eLix0r5bQc8+WtXbMWvu1OR99E9jG6u9F2+9Tkj3dLcBha20A0PznkebnveyzxpjvG2P+0RgzCGxlTs/FWvsK4BtjhpY41s1aPTurPdbt5j9X0OPPljHGBz4A3Mnq70Xb75MG0ty60Vp7OfFGQh7wDx1uj3QHPVcL+wgwxRl2P5IM3YPAucaYFEDzn69pft6TZv7X0FpbBf4R+BHgAPH//gBgjNkIRNbasSWOdbNWz85qj3WtRZ4r6OFnqznouAv4WWttyOrvRdvvU2Kh2xzdexx4d/OjdwP/a609ltQ1z2TGmHXGmA3N33vAzxHfn+8CBWPMDc2vvh/49+bvWx3rWq2endUec9d6t1o8V9Cjz5Yx5kPAa4F3NP9DBKu/F22/T4lu7WiMuZB4+k4RGCeevmMTu+AZzBhzPvAF4r02U8BTwAettUeNMdcTj7LnOTkl5aXmzy16rBsYYz4M/BRwDvAKcNxae3GrZ2e1x852C90r4G0s8lw1f6anni1jzMXAE8CzQLn58QvW2p9c7b1o933SfroiIg5pIE1ExCGFroiIQwpdERGHFLoiIg4pdEVEHFLoirRgjNlnjHlDp9sh3UOhK6vWrkAyxvyrMebPkzj3Wtsh0m4KXRERh7Q4QlbFGPNp4D3EG0UHwJ8R7zX6d8Bu4p2ZfsNa+7XmjkzfBz5grb3LGNNPvFT1z4hX+XyUeB/UGvCgtfZtzY2532etfWCBa98K/DnxZt5PAe+31n6/eWwf8QYn7yVeM/8V4BettZXm8d8Dfqt5vT8G/oV4jf6PtWjHoucTWSn1dGVVrLW/QLwZyNustf3AZ4EvE4fhEPA7wBeMMSPNzUF+GfiX5p63fw88bq39lLX2n5s/+zfW2n5r7dtaXdcYcyXwceB24rdD3AHcaYzJzfnau4A3AduBy4Bfav7sm4DfBt4A7ARumvP3adWOBc8nshoKXWmXnwfuttbeba0NrbX3A98B3gJgrb0P+Dzw38BbiUNzNW4D7rDWfstaG1hrP0nc2752znc+bK090gz7u4Armp+/C/iEtfZJa20J+NNlXnOx84msWKLvSJOesg14pzFmbg8xAzw458//DPwa8BfW2uNruM4vGmN+fc5nWeItHGe8OOf3pTnHXkP8H4IZy93ycbHziayYQlfWYu6AwEHg09ba2xb6YnNv2zuATwEfMMZ8wlr73ALnWcpB4EPW2g+tor1HgR+a8+f5b5TQAIckTqEra/EScH7z958BHjXGvBF4gLiXey3wnLX2EPCHze/9MvD7wKeMMTc2X6kz9zxzZYwx+Tl/bhAPfH3RGPMA8G2gD3gd8A1r7eQS7f134OPNQcD9xANpi/19RBKhmq6sxV8Cf2SMmQB+Fng7cbgeI+6R/i7x+6ReSzyA9d5myP41ca/yD5rn+Riw2xgzYYz5zznnv5t4T9SZX39irf0OcV33H4j3y32OZQ5sWWvvAT5MXPJ4DnikeWhmo+vF2iHSNpoyJj3LGHMR8YbXOWtto9Ptkd6g0JWeYoz5SeKpbeuI3zARWmvf0dlWSS9ReUF6ze3E5Y/niRd1fKCzzZFeo56uiIhD6umKiDik0BURcUihKyLikEJXRMQhha6IiEMKXRERh/4f/yWL/+auVkcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot(data['textLength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data['textLength'] > 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gumshoda aur agwa shoda ashkhas ki bazyabi key leye Beuro aur insedad dehsht gerdi key lea aik comition yeh trust aik gair serkari gair seasi aur gair munfiet andoz insani haqooq ki die tanzem hay aur is ski rejgtration Amrica mein bhi ho chuki hay inn ki mosalsal aur bea louse koshishon sey ab tek lakhon sey zaida aeyse quadi reha ho chukey hain jog air qanooni tour par Pakistan aur barony mumalik mein quaid rahey inn mein sey baz jailon mein hi paida huay they aur chalees chalees sal bea qasoor aur bea gunah sakht moshaqqat ki zindagi mahaz laquniat ki waja sey guzar rahey thay neaz bees hazar sey zaidah gumshudah aur agwa shudah bachchon aur bachion ko baziab karaya ja chukka hay halankey iss hawaley sey unhain khasi tanqeed ka nishana banna para.\n",
      "Insani haqooq key leyeh Ansar Burney ki khidmat key aeteraf mein hukumat Pakistan ki taraf sey unhain sitara e imtaiaz sey nawaza gea aur kai bainul aqwami aizazat bhi unkey hissay mein aeay hain taham jaisa key pehley ziker hua unhain kai halqon ki taraf sey tanqeed ka nishana bhi banaya jata rahta hay shaied iss ki waja yeh hai ke Ansar Burney insani haqooq key leyea kam kertey huay kisi ka sirf Pakistani hona zaruri khaial nahin kertey aur khasusan Pakistani jailon mein band Bharti qaideon ke haq mein awaz buland kerney par unka dosron ki nigah mein qabil nafreen tharna shaiad kisi had tak qabil faham hai.\n",
      "Maroof qanoon dan aur azadi-e-niswan-o-insani haqooq ki alamberdar Asma Jahangir 27 January 1952 ko Lahore mein paida hue in key walid Malik Ghulam Jilani maroof siasat dan they apnay 1968 mein Senior Cambridge kerney key bad 1969 mein kanier college Lahore mein dakhla lea Sadar Ayub key khelaf Awami tahrek uthi to unhon ney bhi college ki talbat ka aik jaloos nikala aur is ski qaiadat ki jis kipadasht mein unhain chand mah key leyeh college sey nikal dea gea issi college sey BA ki degree hasil ki.\n",
      "Yon to Asma jahangir ki puri zindagi hi amriaat aur insani haqooq ki khelaf warzion key khelaf jado jehad kernay se ebarat hai aur unki khidmata ka kae bainul aqwami sithat par eteraf bhi kea ja chukka hay lakin un ka taza tareen karnama November 2007 se March 2009 tak General Pervez Musharraf ki amriat ke khelaf aur bad-e-azan Chief Justics of Pakistan Iftikhar Muhammed Choudhry ki bahali ke haq mein chalae janey wali tahreek mein unka numaya hissa hai iss tehreek mein unki bher pure karkerdagi ka eteraf tamam halqon ki taraf se kea gea magar Chief Justice key haq mein tehreek chalaney ka matleb yea nahin tha key Iftikhar Muhammed Choudhry un key kea kisi ・Moqaddas Gae・ki hasiat ekhtaiar ker jatey yahi waja hay key jab Supreme Court key ful Bench ki taref dey badnama e zamana NRO key khelaf mukhteser faisla jari kea gea to iss faislay key baz nikat par apney tahaffuz ka izhar ker tey huea unhon ney iss par tanqeed bhi ki.\n",
      " App Shaheed huye to app ki jaib se 100 ruppey aik maney order aur do khoon alood khatoot baramad huye Sawar Muhamed Hussain ke walid ne jab unki Shahadat ki khabar suni to pehla sawal yahi kiya ke “mere betey ne maidan jang mein buzdili to nahein dekhai?Sawar Muhammed Hussain Shaheed ka aik emaan bahot mashhoor hai aik dafa aik dost ne ke Sawar Muhammed Hussain Shaheed  walid ke pas 200 rupay batour amanat rakhwa Afroz waqaey issi dauran app ke walid ko bhi rupon ki ashad zaroorat par gaye unhon ne Sawar Muhammed Hussain se ziker kiya to app ne kaha \"Agarchay merey pas zati taur par apnay 50 rupay maujood haien lakin woh kisi ki amanat haien albatta mere pas zati taur par apnay 50 rupay maujood haien app un se kam chala leien mein un 200 rupon mein se aik rupya bhi na doon ga keon ke iss tara amanat mein khayanat hogi aur jab khuda ke hazoor mujhey jawab da hona parey ga to mein wahan kya jawab doon ga?\n",
      " Malala Yousafzai ne zamana e talib e elmi mein jab BBC ki web site par taleem ke masail par apni diary likhna shuru ki to us kiIsi asna mein jab Pak foj ne Sawat mein karwai ki to elaqe se Talban ki ijaradari khatam ho gai, Talban ka supah e salar Fazal Ullah Afghanistan farar ho gaya, larkiyon ke school dobarah khul gaye magar October 2012 mein school jate howae Talban ne taleem ke haq mein awaz buland karne wali kam umar Malala Yousuf  Zai ke sar mein do goliyan dagh den magar qudrat ne usse bacha liya\n",
      " Yahen un ke han 12 December 1948 ko dosra bacha peda howa jis ka naam Iftikhar Muhammad rakha gaya lekin waliden ko kia khabar thi ke aik din ye bacha na sirf un ke leye balke pore mulk ke leye sarmaya e iftikhar ban jaye ga aur aala adliya ka rukh hi nahi, Pakistan ki tarikh badal kar rakh de ga, kon janta tha ke duniya bhar mein Iftikhar Muhammad Caudhary ka naam ehteram se liya jaye ga, America ka national law journal unhen (2007 mein) “lawyer of the year” qarar de ga, Newyork shehar ke wokla ki anjuman unhen ezazi rukniyat pesh karye gi aur Howard law school ki taraf se unhen “tamgha e azadi” (medal of freedom) pesh karte howae yun khiraje tehseen pesh kiya jaye ga “mumtaz qanondan aur musanif (Iftikhar Muhammad Chaudhry) jurat, yaqen e muhqam aur adliya ki azadi ke sath pukhta wabastagi ki badolat ap Pakistan aur duniya bhar mein qanon ki hakmiyat ko barqarar rakhne mein masrof e jehad logon ke leye missal (role model) ban gaye hain\n",
      "Bohot outclass trousers hain! Mujhe toh is tarah ke fitted trousers kabse chahiye thya laken yahan markets mein yaa tou yeh bohot mehnge thay ya tou mil hi nahi rahay thay. Laken daraz.pk par itnay zabardast price mein mere ghar pe deliver kardiye gaye. yeh bohot zabardast hain inki fitting itni achi lagti hai pehn ke, ke kia bataun. Bilkul jis tarah dikhaye gaye hainn ye wese hi hain. Bus ek baat hai ke ye kapra araam de tou hai laken ye garmiyon mein pehnna zara mushkil hai kyunke thorha mota hai yeh kapra.\n",
      "\n",
      "Meine yehee Maximus ki HID do baar pehlay khareedi hui hain, aur kasam se, dono ka experience bohat hee acha tha. Bohat tez aur mazedar HIDs theen yeh aur aisi tez HID lights to meine dekhee bhee nai pehlay.\n",
      "\n",
      "Choaty se gaoun se taluq hai mera, yaha jo bhee hID hotee hain, 400 watt ki keh kar bechte hain , lekin hotee kuch 20 watt ki hain, lekin yeh maximus ki waqai mein 200 watt ki lagtee hai kiunkay bohat bright hai yeh. Mera car modification ka kaam hai aur meray customers ko yeh HID bohat pasand hai. mashaAllah se thoray se customers ne request kee hai inko yehee wali HID chayiey.\n",
      "\n",
      "Sath mein 6 maheenay ki warranty bhee hai, jo meine inko dee hui hai aur unki taraf se bhee kissi tarhan ki complain nai ayee. 6000 lumen hain ismein tou light kuch zyada hee blinding aur tez hai aur issi light ka hee sahee maza hai jab road pe lekay ghumein :D\n",
      "\n",
      "Abdul wali bro nice job yar ap bhoot acha kam kr rhn hain or yaqeen mano main nay webdes k bary main jo kuch seekha pa sa he seekha bhoot umda kam kr rhy hain ap , wali bro laken mujy masla ho rha h word press k sath main website bna le h but os site jasa lay out mujy wordpress main nahi mil rha h navigation bhe ah jati h top wali but main left pa ak navigation chata hn or right pa categories jis main spose krn graphics adob photoshop then photoshop k neacha bhe koey catgory ho jis main main post dal saku plz help me rizwan.isbd@gmail.com mri id h i need help\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in data.loc[data['textLength'] > 500,'text']:\n",
    "    print(line)\n",
    "    i+=1\n",
    "    if i >=10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo:\n",
    "\n",
    "treat emojis like words\n",
    "\n",
    "extract punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "__English Usage__:\n",
    "\n",
    "there is light english usage and some posts are all english\n",
    "list examples\n",
    "\n",
    "__Emoji's__:\n",
    "\n",
    "use of emoji's\n",
    "\n",
    "__Length__:\n",
    "\n",
    "long tail of very long posts. look at how they might affect any embedding creation\n",
    "\n",
    "__Capitalization__:\n",
    "\n",
    "i will make assumption that it's ok to lower case everything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Naive \n",
    "\n",
    "- get top words\n",
    "- derive P(sentiment | word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Text Embedding and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(34)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding using word2vec\n",
    "\n",
    "__Employ Word2Vec in gensim__\n",
    "\n",
    "We set _min count_ (minimum number of word occurences) of 2 as the number of distinct words is small.\n",
    "\n",
    "We scan through setting _size_ (dimension of embedding space) at 20, 50, and 100.\n",
    "\n",
    "We scan through _window_ (words on either side to be considered for content-target pairs) at 2, 5, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cfrailey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parsing__\n",
    "\n",
    "Not know anything about this language or how it is used in social media, we will perform minimal text parsing.\n",
    "\n",
    "We will lower case, tokenize sentences using defaults, tokenize words using defaults, then strip whitespace and punctuation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = \"string. With. Punctuation?\"\n",
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "new_s = s.translate(table)    \n",
    "def UrduParsing(text):\n",
    "    # create list of sentences in text\n",
    "    sentences = nltk.sent_tokenize(text.lower())\n",
    "    # create list of lists of words in sentences and remove whitespace and punctuation\n",
    "    wordsInSentences = [nltk.word_tokenize(s) for s in sentences]\n",
    "    \n",
    "    # loop through words to remove whitespace and punctuation and remove empty words\n",
    "    cleanWordsInSentences = []\n",
    "    table = str.maketrans({key: None for key in string.punctuation+string.whitespace})\n",
    "    for s in wordsInSentences:\n",
    "        cleanSentence = []\n",
    "        for w in s:\n",
    "            cleanWord = w.translate(table)\n",
    "            if len(cleanWord) > 0:\n",
    "                cleanSentence.append(cleanWord)\n",
    "        if len(cleanSentence) > 0:\n",
    "            cleanWordsInSentences.append(cleanSentence)\n",
    "    \n",
    "    return cleanWordsInSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['parsedText'] = data.apply(lambda x: UrduParsing(x['text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>textLength</th>\n",
       "      <th>textWordCount</th>\n",
       "      <th>parsedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ya mere rab tu bra kreem hy is mulk k halat PR...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>94</td>\n",
       "      <td>19</td>\n",
       "      <td>[[ya, mere, rab, tu, bra, kreem, hy, is, mulk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Ap to hero ho  . Autograph do na 😂😂</td>\n",
       "      <td>Positive</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>[[ap, to, hero, ho], [autograph, do, na, 😂😂]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Pskistani jawano, lagatay raho imran khan k do...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "      <td>[[pskistani, jawano, lagatay, raho, imran, kha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Meri jesi kamiyabiyan na karna tum log... go f...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>72</td>\n",
       "      <td>14</td>\n",
       "      <td>[[meri, jesi, kamiyabiyan, na, karna, tum, log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>mere pas kamyaab maa hai. 😂😂😂😂</td>\n",
       "      <td>Positive</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>[[mere, pas, kamyaab, maa, hai], [😂😂😂😂]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>apny apny ami abu ko mri trf sy shukria. Bl dy</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>[[apny, apny, ami, abu, ko, mri, trf, sy, shuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>shukr hy ye namoona peuda kya. Thanks to uncle...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>[[shukr, hy, ye, namoona, peuda, kya], [thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Sana Butt Should i tag your mama? 😂</td>\n",
       "      <td>Positive</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>[[sana, butt, should, i, tag, your, mama], [😂]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Tag them please! !!!😍😍</td>\n",
       "      <td>Positive</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>[[tag, them, please], [😍😍]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Jo chpairh wo maarain agay se uska kya??😯</td>\n",
       "      <td>Positive</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>[[jo, chpairh, wo, maarain, agay, se, uska, ky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>ALLAHTAALA ! Ummat pe Reham Frmay..aor ye lodh...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>128</td>\n",
       "      <td>21</td>\n",
       "      <td>[[allahtaala], [ummat, pe, reham, frmayaor, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Done. 👍 Thanks Urdu Safha’s Admin for such ini...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>[[done], [👍, thanks, urdu, safha, ’, s, admin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Sahil Lodhi ka kya? Ma bap zimadar WO keu apni...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "      <td>[[sahil, lodhi, ka, kya], [ma, bap, zimadar, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Nhi pta ??? Allah or us k rasool s.a.w ki etaa...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>[[nhi, pta], [allah, or, us, k, rasool, saw, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Same thing k Aik bunda Chauti burai ko Sahi sa...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>165</td>\n",
       "      <td>33</td>\n",
       "      <td>[[same, thing, k, aik, bunda, chauti, burai, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>Dosri aeham tip yeh hai ke aap ko apne bachon ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>141</td>\n",
       "      <td>29</td>\n",
       "      <td>[[dosri, aeham, tip, yeh, hai, ke, aap, ko, ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>Asma Jahangir 1995 mein Sadar mumliket ki jani...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>383</td>\n",
       "      <td>64</td>\n",
       "      <td>[[asma, jahangir, 1995, mein, sadar, mumliket,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>2001 mein unhain Aqwam e Muttahida ke tarqiati...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>197</td>\n",
       "      <td>33</td>\n",
       "      <td>[[2001, mein, unhain, aqwam, e, muttahida, ke,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>Bachion ki moshaqqat, begari, unki jabri molaz...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>249</td>\n",
       "      <td>42</td>\n",
       "      <td>[[bachion, ki, moshaqqat, begari, unki, jabri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>Aap ko sakht sadma howa aur kai roz tak aap di...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>197</td>\n",
       "      <td>38</td>\n",
       "      <td>[[aap, ko, sakht, sadma, howa, aur, kai, roz, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Un ki qayadat mein natajurbekar Pakistani khaw...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>253</td>\n",
       "      <td>42</td>\n",
       "      <td>[[un, ki, qayadat, mein, natajurbekar, pakista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>Is tanzeem ka maqsad khawateen ki sahi tor par...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>197</td>\n",
       "      <td>35</td>\n",
       "      <td>[[is, tanzeem, ka, maqsad, khawateen, ki, sahi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>Is project ke tehat ab tak mulk bhar se hazaro...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>235</td>\n",
       "      <td>46</td>\n",
       "      <td>[[is, project, ke, tehat, ab, tak, mulk, bhar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>meine yeh cream apne ghar waloun kay liay khar...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>237</td>\n",
       "      <td>41</td>\n",
       "      <td>[[meine, yeh, cream, apne, ghar, waloun, kay, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>Market sey thora sasta mil raha tha yehan tou ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>195</td>\n",
       "      <td>37</td>\n",
       "      <td>[[market, sey, thora, sasta, mil, raha, tha, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>Es shampoo say bal sarah dil naram rhte ha. Th...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>[[es, shampoo, say, bal, sarah, dil, naram, rh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>Bohat acha perfume hai, iski bottle se lekar i...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>193</td>\n",
       "      <td>36</td>\n",
       "      <td>[[bohat, acha, perfume, hai, iski, bottle, se,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>Abhee tak tou bara acha chal raha hai. Aur isk...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>295</td>\n",
       "      <td>57</td>\n",
       "      <td>[[abhee, tak, tou, bara, acha, chal, raha, hai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>” America ki Nowa South Eastern University ki ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>214</td>\n",
       "      <td>38</td>\n",
       "      <td>[[”, america, ki, nowa, south, eastern, univer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>keh kr Gulzar sahab ne raghib (????) kiya jabk...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>131</td>\n",
       "      <td>23</td>\n",
       "      <td>[[keh, kr, gulzar, sahab, ne, raghib], [kiya, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19877</th>\n",
       "      <td>Sirf punjab ne kaam karana unki pmln ki zimeda...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>121</td>\n",
       "      <td>23</td>\n",
       "      <td>[[sirf, punjab, ne, kaam, karana, unki, pmln, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19878</th>\n",
       "      <td>Lahore gayi ho kabhi ja kar dekho phir peshawa...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>118</td>\n",
       "      <td>22</td>\n",
       "      <td>[[lahore, gayi, ho, kabhi, ja, kar, dekho, phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19879</th>\n",
       "      <td>Harrassment walay cases ko pooray Pakistan mei...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>127</td>\n",
       "      <td>22</td>\n",
       "      <td>[[harrassment, walay, cases, ko, pooray, pakis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19881</th>\n",
       "      <td>Karachi ki baat hi nahe horahi. Lahore mei abh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>324</td>\n",
       "      <td>60</td>\n",
       "      <td>[[karachi, ki, baat, hi, nahe, horahi], [lahor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19891</th>\n",
       "      <td>Inn sabko rokna kiski zimedaari thi? Nawaz aur...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>[[inn, sabko, rokna, kiski, zimedaari, thi], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19907</th>\n",
       "      <td>Benazir achi thi. Zulfiqar ali bhutto acha tha.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>[[benazir, achi, thi], [zulfiqar, ali, bhutto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19911</th>\n",
       "      <td>Mulk ko lootne ke elawa kuch nahe kia dono par...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>[[mulk, ko, lootne, ke, elawa, kuch, nahe, kia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19916</th>\n",
       "      <td>Pehlay metro bus ko jangla bus kehta hai phir ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>112</td>\n",
       "      <td>23</td>\n",
       "      <td>[[pehlay, metro, bus, ko, jangla, bus, kehta, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19928</th>\n",
       "      <td>Aur agar Imran peshawar mei banayega toh 100% ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>121</td>\n",
       "      <td>20</td>\n",
       "      <td>[[aur, agar, imran, peshawar, mei, banayega, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19940</th>\n",
       "      <td>Kursi ke chakkar mai dharnay karta raha yeh ki...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>83</td>\n",
       "      <td>17</td>\n",
       "      <td>[[kursi, ke, chakkar, mai, dharnay, karta, rah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19962</th>\n",
       "      <td>Aapsay baat karne ka koi faidaa nahe. You win.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>[[aapsay, baat, karne, ka, koi, faidaa, nahe],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19964</th>\n",
       "      <td>Bharr mai jao dekhte hain shayad aa bhi jaye ....</td>\n",
       "      <td>Negative</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>[[bharr, mai, jao, dekhte, hain, shayad, aa, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>Aik Aik Eeeeent Punjaab ke apnay hathon se lag...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>81</td>\n",
       "      <td>13</td>\n",
       "      <td>[[aik, aik, eeeeent, punjaab, ke, apnay, hatho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>Us ky bad akhri line sun lay. Phr Imran Khan p...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>62</td>\n",
       "      <td>16</td>\n",
       "      <td>[[us, ky, bad, akhri, line, sun, lay], [phr, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20041</th>\n",
       "      <td>Tum pata hai imran khan ke sath kiun ho??? Pat...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>[[tum, pata, hai, imran, khan, ke, sath, kiun,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>Jisko bta rha. Wo khud hai</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>[[jisko, bta, rha], [wo, khud, hai]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057</th>\n",
       "      <td>Yaar maine tw poori vdo hi nhi dekhi in aunty ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>156</td>\n",
       "      <td>34</td>\n",
       "      <td>[[yaar, maine, tw, poori, vdo, hi, nhi, dekhi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20074</th>\n",
       "      <td>Political disability? Tu aysa insan ko mulk ch...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>116</td>\n",
       "      <td>21</td>\n",
       "      <td>[[political, disability], [tu, aysa, insan, ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20098</th>\n",
       "      <td>Shadi Shadi shadi shadi jahil log its personal...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>83</td>\n",
       "      <td>16</td>\n",
       "      <td>[[shadi, shadi, shadi, shadi, jahil, log, its,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20101</th>\n",
       "      <td>offensive hojayega thoraa. last line best bol ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "      <td>[[offensive, hojayega, thoraa], [last, line, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20104</th>\n",
       "      <td>Wards attend kiye hain? Suna hai wahan bhi is ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>109</td>\n",
       "      <td>20</td>\n",
       "      <td>[[wards, attend, kiye, hain], [suna, hai, waha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20117</th>\n",
       "      <td>PATWARIUN KA KOI HAAL NAHI. JAHIL QOUM SAHEE H...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>653</td>\n",
       "      <td>126</td>\n",
       "      <td>[[patwariun, ka, koi, haal, nahi], [jahil, qou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20119</th>\n",
       "      <td>Why so jazbati panjabiyon?  ye tw hamara falsa...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>[[why, so, jazbati, panjabiyon], [ye, tw, hama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20132</th>\n",
       "      <td>aunty\" kitna bara haath hai??\" lol..</td>\n",
       "      <td>Negative</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>[[aunty, kitna, bara, haath, hai], [lol]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20153</th>\n",
       "      <td>naughty boy! Bhalla corruption karre chup kar ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>77</td>\n",
       "      <td>14</td>\n",
       "      <td>[[naughty, boy], [bhalla, corruption, karre, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20173</th>\n",
       "      <td>Yra is aunty ki bt ghoar say Suni is aunty ki ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>134</td>\n",
       "      <td>31</td>\n",
       "      <td>[[yra, is, aunty, ki, bt, ghoar, say, suni, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20178</th>\n",
       "      <td>Tu ne ek min main puri video sun li ???Behtreen</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>[[tu, ne, ek, min, main, puri, video, sun, li]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180</th>\n",
       "      <td>Mtlb akhir mai no. Dena reh gya :D</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>[[mtlb, akhir, mai, no], [dena, reh, gya, d]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20188</th>\n",
       "      <td>Awam kai sahi mazay a rhy hain! :-D</td>\n",
       "      <td>Positive</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>[[awam, kai, sahi, mazay, a, rhy, hain], [d]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20202</th>\n",
       "      <td>Itni corruption ki apke leader ne demmak ki tr...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>97</td>\n",
       "      <td>19</td>\n",
       "      <td>[[itni, corruption, ki, apke, leader, ne, demm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "10     Ya mere rab tu bra kreem hy is mulk k halat PR...  Positive   \n",
       "106                  Ap to hero ho  . Autograph do na 😂😂  Positive   \n",
       "125    Pskistani jawano, lagatay raho imran khan k do...  Positive   \n",
       "138    Meri jesi kamiyabiyan na karna tum log... go f...  Positive   \n",
       "148                       mere pas kamyaab maa hai. 😂😂😂😂  Positive   \n",
       "288       apny apny ami abu ko mri trf sy shukria. Bl dy  Positive   \n",
       "296    shukr hy ye namoona peuda kya. Thanks to uncle...  Positive   \n",
       "314                  Sana Butt Should i tag your mama? 😂  Positive   \n",
       "379                               Tag them please! !!!😍😍  Positive   \n",
       "381            Jo chpairh wo maarain agay se uska kya??😯  Positive   \n",
       "433    ALLAHTAALA ! Ummat pe Reham Frmay..aor ye lodh...  Positive   \n",
       "434    Done. 👍 Thanks Urdu Safha’s Admin for such ini...  Positive   \n",
       "437    Sahil Lodhi ka kya? Ma bap zimadar WO keu apni...  Positive   \n",
       "453    Nhi pta ??? Allah or us k rasool s.a.w ki etaa...  Positive   \n",
       "462    Same thing k Aik bunda Chauti burai ko Sahi sa...  Positive   \n",
       "965    Dosri aeham tip yeh hai ke aap ko apne bachon ...  Positive   \n",
       "1029   Asma Jahangir 1995 mein Sadar mumliket ki jani...  Positive   \n",
       "1030   2001 mein unhain Aqwam e Muttahida ke tarqiati...  Positive   \n",
       "1031   Bachion ki moshaqqat, begari, unki jabri molaz...  Positive   \n",
       "1042   Aap ko sakht sadma howa aur kai roz tak aap di...  Positive   \n",
       "1057   Un ki qayadat mein natajurbekar Pakistani khaw...  Positive   \n",
       "1064   Is tanzeem ka maqsad khawateen ki sahi tor par...  Positive   \n",
       "1094   Is project ke tehat ab tak mulk bhar se hazaro...  Positive   \n",
       "1191   meine yeh cream apne ghar waloun kay liay khar...  Positive   \n",
       "1193   Market sey thora sasta mil raha tha yehan tou ...  Positive   \n",
       "1194   Es shampoo say bal sarah dil naram rhte ha. Th...  Positive   \n",
       "1197   Bohat acha perfume hai, iski bottle se lekar i...  Positive   \n",
       "1201   Abhee tak tou bara acha chal raha hai. Aur isk...  Positive   \n",
       "1268   ” America ki Nowa South Eastern University ki ...  Positive   \n",
       "1560   keh kr Gulzar sahab ne raghib (????) kiya jabk...  Positive   \n",
       "...                                                  ...       ...   \n",
       "19877  Sirf punjab ne kaam karana unki pmln ki zimeda...  Negative   \n",
       "19878  Lahore gayi ho kabhi ja kar dekho phir peshawa...  Negative   \n",
       "19879  Harrassment walay cases ko pooray Pakistan mei...  Negative   \n",
       "19881  Karachi ki baat hi nahe horahi. Lahore mei abh...  Negative   \n",
       "19891  Inn sabko rokna kiski zimedaari thi? Nawaz aur...  Negative   \n",
       "19907    Benazir achi thi. Zulfiqar ali bhutto acha tha.  Positive   \n",
       "19911  Mulk ko lootne ke elawa kuch nahe kia dono par...  Negative   \n",
       "19916  Pehlay metro bus ko jangla bus kehta hai phir ...  Negative   \n",
       "19928  Aur agar Imran peshawar mei banayega toh 100% ...  Positive   \n",
       "19940  Kursi ke chakkar mai dharnay karta raha yeh ki...  Negative   \n",
       "19962     Aapsay baat karne ka koi faidaa nahe. You win.  Negative   \n",
       "19964  Bharr mai jao dekhte hain shayad aa bhi jaye ....  Negative   \n",
       "19975  Aik Aik Eeeeent Punjaab ke apnay hathon se lag...   Neutral   \n",
       "20004  Us ky bad akhri line sun lay. Phr Imran Khan p...  Positive   \n",
       "20041  Tum pata hai imran khan ke sath kiun ho??? Pat...   Neutral   \n",
       "20051                         Jisko bta rha. Wo khud hai   Neutral   \n",
       "20057  Yaar maine tw poori vdo hi nhi dekhi in aunty ...  Negative   \n",
       "20074  Political disability? Tu aysa insan ko mulk ch...  Negative   \n",
       "20098  Shadi Shadi shadi shadi jahil log its personal...  Negative   \n",
       "20101  offensive hojayega thoraa. last line best bol ...   Neutral   \n",
       "20104  Wards attend kiye hain? Suna hai wahan bhi is ...  Negative   \n",
       "20117  PATWARIUN KA KOI HAAL NAHI. JAHIL QOUM SAHEE H...  Negative   \n",
       "20119  Why so jazbati panjabiyon?  ye tw hamara falsa...  Negative   \n",
       "20132               aunty\" kitna bara haath hai??\" lol..  Negative   \n",
       "20153  naughty boy! Bhalla corruption karre chup kar ...  Negative   \n",
       "20173  Yra is aunty ki bt ghoar say Suni is aunty ki ...  Negative   \n",
       "20178    Tu ne ek min main puri video sun li ???Behtreen   Neutral   \n",
       "20180                 Mtlb akhir mai no. Dena reh gya :D   Neutral   \n",
       "20188                Awam kai sahi mazay a rhy hain! :-D  Positive   \n",
       "20202  Itni corruption ki apke leader ne demmak ki tr...  Negative   \n",
       "\n",
       "       textLength  textWordCount  \\\n",
       "10             94             19   \n",
       "106            35             10   \n",
       "125            91             16   \n",
       "138            72             14   \n",
       "148            30              6   \n",
       "288            46             11   \n",
       "296            56             11   \n",
       "314            35              8   \n",
       "379            22              4   \n",
       "381            41              8   \n",
       "433           128             21   \n",
       "434            54              9   \n",
       "437            84             18   \n",
       "453            53             12   \n",
       "462           165             33   \n",
       "965           141             29   \n",
       "1029          383             64   \n",
       "1030          197             33   \n",
       "1031          249             42   \n",
       "1042          197             38   \n",
       "1057          253             42   \n",
       "1064          197             35   \n",
       "1094          235             46   \n",
       "1191          237             41   \n",
       "1193          195             37   \n",
       "1194           56             11   \n",
       "1197          193             36   \n",
       "1201          295             57   \n",
       "1268          214             38   \n",
       "1560          131             23   \n",
       "...           ...            ...   \n",
       "19877         121             23   \n",
       "19878         118             22   \n",
       "19879         127             22   \n",
       "19881         324             60   \n",
       "19891          72             13   \n",
       "19907          47              8   \n",
       "19911          84             17   \n",
       "19916         112             23   \n",
       "19928         121             20   \n",
       "19940          83             17   \n",
       "19962          46              9   \n",
       "19964          86             19   \n",
       "19975          81             13   \n",
       "20004          62             16   \n",
       "20041          61             13   \n",
       "20051          26              6   \n",
       "20057         156             34   \n",
       "20074         116             21   \n",
       "20098          83             16   \n",
       "20101          71             12   \n",
       "20104         109             20   \n",
       "20117         653            126   \n",
       "20119          53             11   \n",
       "20132          36              6   \n",
       "20153          77             14   \n",
       "20173         134             31   \n",
       "20178          47             10   \n",
       "20180          34              8   \n",
       "20188          35              8   \n",
       "20202          97             19   \n",
       "\n",
       "                                              parsedText  \n",
       "10     [[ya, mere, rab, tu, bra, kreem, hy, is, mulk,...  \n",
       "106        [[ap, to, hero, ho], [autograph, do, na, 😂😂]]  \n",
       "125    [[pskistani, jawano, lagatay, raho, imran, kha...  \n",
       "138    [[meri, jesi, kamiyabiyan, na, karna, tum, log...  \n",
       "148             [[mere, pas, kamyaab, maa, hai], [😂😂😂😂]]  \n",
       "288    [[apny, apny, ami, abu, ko, mri, trf, sy, shuk...  \n",
       "296    [[shukr, hy, ye, namoona, peuda, kya], [thanks...  \n",
       "314      [[sana, butt, should, i, tag, your, mama], [😂]]  \n",
       "379                          [[tag, them, please], [😍😍]]  \n",
       "381    [[jo, chpairh, wo, maarain, agay, se, uska, ky...  \n",
       "433    [[allahtaala], [ummat, pe, reham, frmayaor, ye...  \n",
       "434    [[done], [👍, thanks, urdu, safha, ’, s, admin,...  \n",
       "437    [[sahil, lodhi, ka, kya], [ma, bap, zimadar, w...  \n",
       "453    [[nhi, pta], [allah, or, us, k, rasool, saw, k...  \n",
       "462    [[same, thing, k, aik, bunda, chauti, burai, k...  \n",
       "965    [[dosri, aeham, tip, yeh, hai, ke, aap, ko, ap...  \n",
       "1029   [[asma, jahangir, 1995, mein, sadar, mumliket,...  \n",
       "1030   [[2001, mein, unhain, aqwam, e, muttahida, ke,...  \n",
       "1031   [[bachion, ki, moshaqqat, begari, unki, jabri,...  \n",
       "1042   [[aap, ko, sakht, sadma, howa, aur, kai, roz, ...  \n",
       "1057   [[un, ki, qayadat, mein, natajurbekar, pakista...  \n",
       "1064   [[is, tanzeem, ka, maqsad, khawateen, ki, sahi...  \n",
       "1094   [[is, project, ke, tehat, ab, tak, mulk, bhar,...  \n",
       "1191   [[meine, yeh, cream, apne, ghar, waloun, kay, ...  \n",
       "1193   [[market, sey, thora, sasta, mil, raha, tha, y...  \n",
       "1194   [[es, shampoo, say, bal, sarah, dil, naram, rh...  \n",
       "1197   [[bohat, acha, perfume, hai, iski, bottle, se,...  \n",
       "1201   [[abhee, tak, tou, bara, acha, chal, raha, hai...  \n",
       "1268   [[”, america, ki, nowa, south, eastern, univer...  \n",
       "1560   [[keh, kr, gulzar, sahab, ne, raghib], [kiya, ...  \n",
       "...                                                  ...  \n",
       "19877  [[sirf, punjab, ne, kaam, karana, unki, pmln, ...  \n",
       "19878  [[lahore, gayi, ho, kabhi, ja, kar, dekho, phi...  \n",
       "19879  [[harrassment, walay, cases, ko, pooray, pakis...  \n",
       "19881  [[karachi, ki, baat, hi, nahe, horahi], [lahor...  \n",
       "19891  [[inn, sabko, rokna, kiski, zimedaari, thi], [...  \n",
       "19907  [[benazir, achi, thi], [zulfiqar, ali, bhutto,...  \n",
       "19911  [[mulk, ko, lootne, ke, elawa, kuch, nahe, kia...  \n",
       "19916  [[pehlay, metro, bus, ko, jangla, bus, kehta, ...  \n",
       "19928  [[aur, agar, imran, peshawar, mei, banayega, t...  \n",
       "19940  [[kursi, ke, chakkar, mai, dharnay, karta, rah...  \n",
       "19962  [[aapsay, baat, karne, ka, koi, faidaa, nahe],...  \n",
       "19964  [[bharr, mai, jao, dekhte, hain, shayad, aa, b...  \n",
       "19975  [[aik, aik, eeeeent, punjaab, ke, apnay, hatho...  \n",
       "20004  [[us, ky, bad, akhri, line, sun, lay], [phr, i...  \n",
       "20041  [[tum, pata, hai, imran, khan, ke, sath, kiun,...  \n",
       "20051               [[jisko, bta, rha], [wo, khud, hai]]  \n",
       "20057  [[yaar, maine, tw, poori, vdo, hi, nhi, dekhi,...  \n",
       "20074  [[political, disability], [tu, aysa, insan, ko...  \n",
       "20098  [[shadi, shadi, shadi, shadi, jahil, log, its,...  \n",
       "20101  [[offensive, hojayega, thoraa], [last, line, b...  \n",
       "20104  [[wards, attend, kiye, hain], [suna, hai, waha...  \n",
       "20117  [[patwariun, ka, koi, haal, nahi], [jahil, qou...  \n",
       "20119  [[why, so, jazbati, panjabiyon], [ye, tw, hama...  \n",
       "20132          [[aunty, kitna, bara, haath, hai], [lol]]  \n",
       "20153  [[naughty, boy], [bhalla, corruption, karre, c...  \n",
       "20173  [[yra, is, aunty, ki, bt, ghoar, say, suni, is...  \n",
       "20178  [[tu, ne, ek, min, main, puri, video, sun, li]...  \n",
       "20180      [[mtlb, akhir, mai, no], [dena, reh, gya, d]]  \n",
       "20188      [[awam, kai, sahi, mazay, a, rhy, hain], [d]]  \n",
       "20202  [[itni, corruption, ki, apke, leader, ne, demm...  \n",
       "\n",
       "[943 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['parsedText'].apply(len)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of parsed sentences to pass to Word2Vec\n",
    "words_in_sentences = []\n",
    "for l in data['parsedText']:\n",
    "    words_in_sentences += l\n",
    "\n",
    "# Create Embeddings for sizes of 20, 50, 100 and windows of 2,5,7\n",
    "sizes = [10,50,100]\n",
    "windows = [2,5,7]\n",
    "\n",
    "word2vec = {(s,w): Word2Vec(words_in_sentences, min_count = 2, size = s, seed = 28, window = w) \n",
    "            for (s,w) in itertools.product(sizes,windows)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Naive center of Mass Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerOfWords(words_in_sentences, vocab, w2v):\n",
    "    \n",
    "    embed_dim = w2v[random.sample(vocab,1)].shape[1]\n",
    "    \n",
    "    center = np.zeros(embed_dim)\n",
    "    \n",
    "    for s in words_in_sentences:\n",
    "        for w in s:\n",
    "            if w in vocab:\n",
    "                center += w2v[w]\n",
    "    \n",
    "    if np.count_nonzero(center) == 0:\n",
    "        return np.ones(embed_dim)/np.sqrt(np.dot(np.ones(embed_dim),np.ones(embed_dim)))\n",
    "    \n",
    "    else:\n",
    "        return center/np.sqrt(np.dot(center,center))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find Average of Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorify(sentiment):\n",
    "    if sentiment == 'Neutral':\n",
    "        return [0,1,0]\n",
    "    elif sentiment == 'Positive':\n",
    "        return [0,0,1]\n",
    "    elif sentiment == 'Negative':\n",
    "        return [1,0,0]\n",
    "    else:\n",
    "        raise('Inappropriate Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(x):\n",
    "    return np.array([categorify(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = Y.shape[1]                                # number of classes  \n",
    "    n_h = X.shape[1]                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_h, n_y) / np.sqrt(n_h)\n",
    "    b = np.zeros((1,n_y))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        tot_cost = 0\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = np.dot(X[i,:],W) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -1*np.sum(Y[i,:]*np.log(a))\n",
    "            \n",
    "            tot_cost+= cost\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y[i,:]\n",
    "            dW = np.dot(X[i,:].reshape(n_h,1),dz.reshape(1,n_y))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(tot_cost))\n",
    "        \n",
    "    print(\"Epoch: \" + str(t) + \" --- cost = \" + str(tot_cost))\n",
    "\n",
    "    return [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pT_train, pT_test, s_train, s_test = train_test_split(data['parsedText'], data['sentiment'], \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRuns = {}\n",
    "for (s, w, l) in itertools.product(sizes, windows, learningRates):\n",
    "    modelRuns[s,w,l] = [word2vec[(s,w)].wv] + list(modelResults[(s,w,l)])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 17533.992364608835\n",
      "Epoch: 999 --- cost = 17346.875278585107\n",
      "Epoch: 0 --- cost = 17261.817790994897\n",
      "Epoch: 999 --- cost = 16874.6849825992\n",
      "Epoch: 0 --- cost = 17431.930653476433\n",
      "Epoch: 999 --- cost = 16838.904193806928\n",
      "Epoch: 0 --- cost = 17483.342614239304\n",
      "Epoch: 999 --- cost = 17308.852115028152\n",
      "Epoch: 0 --- cost = 17219.11287076605\n",
      "Epoch: 999 --- cost = 16836.714653427847\n",
      "Epoch: 0 --- cost = 17412.7192800266\n",
      "Epoch: 999 --- cost = 16799.307741089116\n",
      "Epoch: 0 --- cost = 17459.92452948505\n",
      "Epoch: 999 --- cost = 17217.449610871678\n",
      "Epoch: 0 --- cost = 17196.46057327386\n",
      "Epoch: 999 --- cost = 16748.13964183978\n",
      "Epoch: 0 --- cost = 17398.465466748155\n",
      "Epoch: 999 --- cost = 16712.220406830398\n",
      "Epoch: 0 --- cost = 17547.12543034848\n",
      "Epoch: 999 --- cost = 16827.00846808831\n",
      "Epoch: 0 --- cost = 17284.729113804588\n",
      "Epoch: 999 --- cost = 16484.531946592888\n",
      "Epoch: 0 --- cost = 17395.991819993797\n",
      "Epoch: 999 --- cost = 16721.190320803013\n",
      "Epoch: 0 --- cost = 17515.860698669472\n",
      "Epoch: 999 --- cost = 16796.482724549798\n",
      "Epoch: 0 --- cost = 17259.330584026502\n",
      "Epoch: 999 --- cost = 16485.87827601647\n",
      "Epoch: 0 --- cost = 17376.105200466223\n",
      "Epoch: 999 --- cost = 16732.040952033763\n",
      "Epoch: 0 --- cost = 17503.027069281474\n",
      "Epoch: 999 --- cost = 16760.189967761737\n",
      "Epoch: 0 --- cost = 17245.959720613013\n",
      "Epoch: 999 --- cost = 16450.515426189366\n",
      "Epoch: 0 --- cost = 17381.652536480626\n",
      "Epoch: 999 --- cost = 16697.27602054012\n",
      "Epoch: 0 --- cost = 17582.226066028466\n",
      "Epoch: 999 --- cost = 16451.994771872498\n",
      "Epoch: 0 --- cost = 17304.876572751575\n",
      "Epoch: 999 --- cost = 16435.426399434873\n",
      "Epoch: 0 --- cost = 17403.281876476816\n",
      "Epoch: 999 --- cost = 16777.91091060955\n",
      "Epoch: 0 --- cost = 17556.664180547938\n",
      "Epoch: 999 --- cost = 16484.218940976843\n",
      "Epoch: 0 --- cost = 17281.844130248865\n",
      "Epoch: 999 --- cost = 16486.441331791655\n",
      "Epoch: 0 --- cost = 17394.449844887415\n",
      "Epoch: 999 --- cost = 16809.896440999593\n",
      "Epoch: 0 --- cost = 17538.83694006229\n",
      "Epoch: 999 --- cost = 16447.90535474877\n",
      "Epoch: 0 --- cost = 17263.115990288596\n",
      "Epoch: 999 --- cost = 16397.694373844668\n",
      "Epoch: 0 --- cost = 17388.529292639625\n",
      "Epoch: 999 --- cost = 16752.84390057163\n"
     ]
    }
   ],
   "source": [
    "learningRates = [.1,.01,.001]\n",
    "modelResults = {}\n",
    "\n",
    "Y = np.array([categorify(s_train.iloc[i]) for i in range(s_train.shape[0])])\n",
    "\n",
    "for (s, w, l) in itertools.product(sizes, windows, learningRates):\n",
    "    wv = word2vec[(s,w)].wv\n",
    "    X = np.array([centerOfWords(pT_train.iloc[i], set(wv.vocab.keys()), wv) \n",
    "                  for i in range(pT_train.shape[0])])\n",
    "    modelResults[(s,w,l)] = model(X, Y, learning_rate = l, num_iterations = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('obj/AmazonModelResultsDict.pkl','wb') as f:\n",
    "    pickle.dump(modelResults,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('obj/AmazonModelRunsDict.pkl','wb') as f:\n",
    "    pickle.dump(modelRuns,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropNaiveModel(X,W,b):\n",
    "    Z = np.atleast_2d(np.dot(X,W) + b)\n",
    "    A = np.exp(Z)/np.atleast_2d(np.sum(np.exp(Z), axis = 1)).T\n",
    "    return(A)\n",
    "\n",
    "def predictNaiveModel(X,W,b,weights):\n",
    "    A = forwardPropNaiveModel(X,W,b)*weights\n",
    "    pred = np.argmax(A,axis=1)\n",
    "    return pred\n",
    "\n",
    "def prepXY(texts, sentiments, wv):\n",
    "    X = np.array([centerOfWords(texts.iloc[i], set(wv.vocab.keys()), wv) for i in range(texts.shape[0])])\n",
    "    Y = np.array([categorify(sentiments.iloc[i]) for i in range(sentiments.shape[0])])\n",
    "    return X,Y\n",
    "\n",
    "def evaluateNaiveModel(texts, sentiments, wv, W, b, weights = np.reshape(np.array([1,1,1]),(1,3))):\n",
    "    X,Y = prepXY(texts, sentiments, wv)\n",
    "    Ylabels = np.argmax(Y,axis = 1)\n",
    "    Yhat = predictNaiveModel(X,W,b,weights)\n",
    "    confMat = LazyConfMat(Ylabels,Yhat)\n",
    "    return confMat,Ylabels,Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LazyConfMat(Ylabels, Yhat):\n",
    "    confMat = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            confMat[i,j] = sum((Ylabels[:] == i) * (Yhat[:] == j))\n",
    "            \n",
    "    return confMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 2, learning rate 0.1 the conf mat is \n",
      "[[ 840.   79.  113.]\n",
      " [1191.  368.  273.]\n",
      " [ 717.  128.  337.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 2, learning rate 0.01 the conf mat is \n",
      "[[  38.  786.  208.]\n",
      " [  60. 1369.  403.]\n",
      " [  38.  671.  473.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 2, learning rate 0.001 the conf mat is \n",
      "[[2.000e+00 8.880e+02 1.420e+02]\n",
      " [4.000e+00 1.516e+03 3.120e+02]\n",
      " [1.000e+00 8.220e+02 3.590e+02]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 5, learning rate 0.1 the conf mat is \n",
      "[[ 844.   56.  132.]\n",
      " [1194.  314.  324.]\n",
      " [ 721.   98.  363.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 5, learning rate 0.01 the conf mat is \n",
      "[[  38.  744.  250.]\n",
      " [  80. 1263.  489.]\n",
      " [  38.  618.  526.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 5, learning rate 0.001 the conf mat is \n",
      "[[   5.  844.  183.]\n",
      " [   8. 1419.  405.]\n",
      " [   2.  756.  424.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 7, learning rate 0.1 the conf mat is \n",
      "[[ 846.  104.   82.]\n",
      " [1135.  485.  212.]\n",
      " [ 727.  207.  248.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 7, learning rate 0.01 the conf mat is \n",
      "[[ 119.  650.  263.]\n",
      " [ 156. 1231.  445.]\n",
      " [  99.  561.  522.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 10, window 7, learning rate 0.001 the conf mat is \n",
      "[[  16.  818.  198.]\n",
      " [  30. 1440.  362.]\n",
      " [  11.  745.  426.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 2, learning rate 0.1 the conf mat is \n",
      "[[773. 153. 106.]\n",
      " [958. 593. 281.]\n",
      " [626. 185. 371.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 2, learning rate 0.01 the conf mat is \n",
      "[[ 115.  697.  220.]\n",
      " [ 107. 1348.  377.]\n",
      " [  70.  613.  499.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 2, learning rate 0.001 the conf mat is \n",
      "[[   4.  880.  148.]\n",
      " [  12. 1547.  273.]\n",
      " [   2.  815.  365.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 5, learning rate 0.1 the conf mat is \n",
      "[[ 795.   93.  144.]\n",
      " [1002.  491.  339.]\n",
      " [ 609.  127.  446.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 5, learning rate 0.01 the conf mat is \n",
      "[[ 115.  675.  242.]\n",
      " [ 119. 1294.  419.]\n",
      " [  67.  578.  537.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 5, learning rate 0.001 the conf mat is \n",
      "[[   0.  867.  165.]\n",
      " [   4. 1495.  333.]\n",
      " [   0.  765.  417.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 7, learning rate 0.1 the conf mat is \n",
      "[[ 835.   73.  124.]\n",
      " [1070.  457.  305.]\n",
      " [ 651.  120.  411.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 7, learning rate 0.01 the conf mat is \n",
      "[[ 131.  673.  228.]\n",
      " [ 109. 1302.  421.]\n",
      " [  85.  566.  531.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 50, window 7, learning rate 0.001 the conf mat is \n",
      "[[   2.  868.  162.]\n",
      " [   6. 1477.  349.]\n",
      " [   0.  762.  420.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 2, learning rate 0.1 the conf mat is \n",
      "[[790.  98. 144.]\n",
      " [967. 513. 352.]\n",
      " [573. 111. 498.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 2, learning rate 0.01 the conf mat is \n",
      "[[ 110.  698.  224.]\n",
      " [ 120. 1352.  360.]\n",
      " [  75.  597.  510.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 2, learning rate 0.001 the conf mat is \n",
      "[[1.000e+00 8.880e+02 1.430e+02]\n",
      " [1.000e+00 1.561e+03 2.700e+02]\n",
      " [1.000e+00 8.400e+02 3.410e+02]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 5, learning rate 0.1 the conf mat is \n",
      "[[ 830.   53.  149.]\n",
      " [1044.  430.  358.]\n",
      " [ 583.   97.  502.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 5, learning rate 0.01 the conf mat is \n",
      "[[  64.  732.  236.]\n",
      " [  73. 1342.  417.]\n",
      " [  38.  591.  553.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 5, learning rate 0.001 the conf mat is \n",
      "[[   0.  890.  142.]\n",
      " [   0. 1486.  346.]\n",
      " [   0.  785.  397.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 7, learning rate 0.1 the conf mat is \n",
      "[[790.  85. 157.]\n",
      " [973. 513. 346.]\n",
      " [543. 137. 502.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 7, learning rate 0.01 the conf mat is \n",
      "[[ 131.  652.  249.]\n",
      " [ 128. 1284.  420.]\n",
      " [  85.  554.  543.]]\n",
      "<class 'numpy.ndarray'>\n",
      "for size 100, window 7, learning rate 0.001 the conf mat is \n",
      "[[1.000e+00 8.740e+02 1.570e+02]\n",
      " [2.000e+00 1.487e+03 3.430e+02]\n",
      " [0.000e+00 7.700e+02 4.120e+02]]\n"
     ]
    }
   ],
   "source": [
    "# pT_train, pT_test, s_train, s_test\n",
    "\n",
    "for s,w,l in itertools.product(sizes,windows,learningRates):\n",
    "    wv, W, b = modelRuns[(s,w,l)]\n",
    "    cf,_,_ = evaluateNaiveModel(pT_test, s_test, wv, W, b)\n",
    "    print(\"for size {}, window {}, learning rate {} the conf mat is \".format(str(s),str(w),str(l)))\n",
    "    print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([categorify(s_train.iloc[i]) for i in range(s_train.shape[0])])\n",
    "\n",
    "for (s, w, l) in itertools.product(sizes, windows, learningRates):\n",
    "    wv = word2vec[(s,w)].wv\n",
    "    X = np.array([centerOfWords(pT_train.iloc[i], set(wv.vocab.keys()), wv) \n",
    "                  for i in range(pT_train.shape[0])])\n",
    "    modelResults[(s,w,l)] = model(X, Y, learning_rate = l, num_iterations = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[1,1],[2,3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c == 1) * (c == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11000,     0,     0,     0,     0,  2233,     0,     0,     0,\n",
       "         2950]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for \n",
    "np.histogram(modelResults[(50,7,.1)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([centerOfWords(data.iloc[i,4], set(word2vec[(50,5)].wv.vocab.keys()), word2vec[(50,5)].wv) \n",
    "              for i in range(data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([categorify(data.iloc[i,1]) for i in range(data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
